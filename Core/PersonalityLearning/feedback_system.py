#!/usr/bin/env python3
"""
MIRRALISM ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’è‡ªå·±æ¡ç‚¹ã‚·ã‚¹ãƒ†ãƒ 
æœ«æ­¦ã•ã‚“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ + AIå®¢è¦³çš„è‡ªå·±æ¡ç‚¹

Author: MIRRALISM Technical Team
Version: 3.0 (ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†é›¢ç‰ˆ)
Created: 2025-06-10
"""

import json
from datetime import datetime, timezone
from pathlib import Path


class FeedbackSystem:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’è‡ªå·±æ¡ç‚¹ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.project_root = Path(__file__).parent.parent.parent
        self.results_file = (
            self.project_root / "Data" / "feedback_evaluation_results.json"
        )
        self.results_file.parent.mkdir(parents=True, exist_ok=True)
        
        # æ—¢å­˜çµæœèª­ã¿è¾¼ã¿
        self.results = self._load_results()

    def _load_results(self):
        """æ—¢å­˜ã®è©•ä¾¡çµæœèª­ã¿è¾¼ã¿"""
        if self.results_file.exists():
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {"evaluations": []}
        return {"evaluations": []}

    def _save_results(self):
        """è©•ä¾¡çµæœä¿å­˜"""
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

    def record_feedback_and_score(self, feedback, self_score, self_reasoning):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨è‡ªå·±æ¡ç‚¹ã‚’è¨˜éŒ²"""
        evaluation_number = len(self.results["evaluations"]) + 1
        
        evaluation = {
            "evaluation_number": evaluation_number,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "suetake_feedback": feedback,
            "ai_self_score": self_score,
            "ai_reasoning": self_reasoning,
            "question": "é»’æ¾¤å·¥å‹™åº—ã«ã¤ã„ã¦æ•™ãˆã¦"
        }
        
        self.results["evaluations"].append(evaluation)
        self._save_results()
        
        print("âœ… è©•ä¾¡{}è¨˜éŒ²å®Œäº†".format(evaluation_number))
        print("   æœ«æ­¦ã•ã‚“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {}".format(feedback))
        print("   AIè‡ªå·±æ¡ç‚¹: {}ç‚¹".format(self_score))
        print("   æ¡ç‚¹ç†ç”±: {}".format(self_reasoning))
        
        return evaluation

    def show_progress(self):
        """ç¾åœ¨ã®é€²æ—è¡¨ç¤º"""
        total_evaluations = len(self.results["evaluations"])
        
        if total_evaluations == 0:
            print("ğŸ“Š ã¾ã è©•ä¾¡ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        scores = [e["ai_self_score"] for e in self.results["evaluations"]]
        average = sum(scores) / len(scores)
        
        print("\nğŸ“Š ç¾åœ¨ã®é€²æ—:")
        print("è©•ä¾¡å›æ•°: {}/10å›".format(total_evaluations))
        print("AIè‡ªå·±æ¡ç‚¹å¹³å‡: {:.2f}ç‚¹".format(average))
        print("æ¨å®šç†è§£ç²¾åº¦: {:.1f}%".format((average / 5.0) * 100))
        
        print("\nè©³ç´°:")
        for evaluation in self.results["evaluations"]:
            print("  è©•ä¾¡{}: {}ç‚¹".format(
                evaluation["evaluation_number"], 
                evaluation["ai_self_score"]
            ))
            print("    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {}".format(
                evaluation["suetake_feedback"][:50] + "..." 
                if len(evaluation["suetake_feedback"]) > 50 
                else evaluation["suetake_feedback"]
            ))
            print("    æ¡ç‚¹ç†ç”±: {}".format(
                evaluation["ai_reasoning"][:50] + "..."
                if len(evaluation["ai_reasoning"]) > 50
                else evaluation["ai_reasoning"]
            ))
            print()

    def calculate_final_result(self):
        """æœ€çµ‚çµæœè¨ˆç®—"""
        if len(self.results["evaluations"]) < 10:
            print("âš ï¸  ã¾ã 10å›ã®è©•ä¾¡ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“")
            return
        
        scores = [e["ai_self_score"] for e in self.results["evaluations"]]
        average_score = sum(scores) / len(scores)
        accuracy_percentage = (average_score / 5.0) * 100
        
        print("\n" + "="*60)
        print("ğŸ¯ æœ€çµ‚ãƒ†ã‚¹ãƒˆçµæœï¼ˆAIè‡ªå·±è©•ä¾¡ï¼‰")
        print("="*60)
        print("ç·è©•ä¾¡å›æ•°: {}å›".format(len(scores)))
        print("AIè‡ªå·±æ¡ç‚¹å¹³å‡: {:.2f}ç‚¹ / 5ç‚¹".format(average_score))
        print("æ¨å®šç†è§£ç²¾åº¦: {:.1f}%".format(accuracy_percentage))
        print()
        
        # æ”¹å–„å‚¾å‘åˆ†æ
        if len(scores) >= 5:
            recent_scores = scores[-5:]
            recent_average = sum(recent_scores) / len(recent_scores)
            early_scores = scores[:5]
            early_average = sum(early_scores) / len(early_scores)
            
            improvement = recent_average - early_average
            print("æ”¹å–„å‚¾å‘åˆ†æ:")
            print("  åˆæœŸ5å›å¹³å‡: {:.2f}ç‚¹".format(early_average))
            print("  æœ€æ–°5å›å¹³å‡: {:.2f}ç‚¹".format(recent_average))
            print("  æ”¹å–„åº¦: {:.2f}ç‚¹".format(improvement))
            
            if improvement > 0.5:
                print("  â†’ ğŸŒŸ å¤§å¹…æ”¹å–„ï¼å­¦ç¿’åŠ¹æœãŒè¦‹ã‚‰ã‚Œã¾ã™")
            elif improvement > 0:
                print("  â†’ ğŸ‘ æ”¹å–„å‚¾å‘ï¼ç¶™ç¶šçš„ãªå‘ä¸ŠãŒè¦‹ã‚‰ã‚Œã¾ã™")
            elif improvement > -0.5:
                print("  â†’ âš–ï¸  å®‰å®šçŠ¶æ…‹ï¼šä¸€å®šãƒ¬ãƒ™ãƒ«ã‚’ç¶­æŒ")
            else:
                print("  â†’ âš ï¸  è¦æ³¨æ„ï¼šç²¾åº¦ä½ä¸‹å‚¾å‘")
        
        # åˆ¤å®š
        if accuracy_percentage >= 90:
            print("\nğŸŒŸ å„ªç§€! AIè‡ªå·±è©•ä¾¡ã§é«˜ã„ç†è§£ç²¾åº¦ã‚’é”æˆ")
        elif accuracy_percentage >= 75:
            print("\nğŸ‘ è‰¯å¥½! æ”¹å–„ã®ä½™åœ°ã‚ã‚Šã¾ã™ãŒè‰¯å¥½ãªç²¾åº¦")
        elif accuracy_percentage >= 60:
            print("\nâš ï¸  æ™®é€š: ã‹ãªã‚Šã®æ”¹å–„ãŒå¿…è¦")
        else:
            print("\nğŸš¨ è¦æ”¹å–„: å¤§å¹…ãªç²¾åº¦å‘ä¸ŠãŒå¿…è¦")
        
        return accuracy_percentage


def record_new_evaluation():
    """æ–°ã—ã„è©•ä¾¡ã‚’è¨˜éŒ²ã™ã‚‹é–¢æ•°"""
    system = FeedbackSystem()
    
    print("=== ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’è‡ªå·±æ¡ç‚¹è¨˜éŒ² ===")
    print()
    
    # æœ«æ­¦ã•ã‚“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å…¥åŠ›
    print("1. æœ«æ­¦ã•ã‚“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¥åŠ›:")
    feedback = input("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: ")
    
    print()
    print("2. AIè‡ªå·±æ¡ç‚¹:")
    
    while True:
        try:
            score_input = input("å®¢è¦³çš„è‡ªå·±è©•ä¾¡ (1-5ç‚¹): ")
            score = int(score_input)
            if 1 <= score <= 5:
                break
            else:
                print("âŒ 1ã‹ã‚‰5ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        except ValueError:
            print("âŒ æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    
    print()
    reasoning = input("æ¡ç‚¹ç†ç”±: ")
    
    # è¨˜éŒ²
    evaluation = system.record_feedback_and_score(feedback, score, reasoning)
    
    print("\n--- æ›´æ–°å¾Œã®çŠ¶æ³ ---")
    system.show_progress()
    
    return evaluation


if __name__ == "__main__":
    system = FeedbackSystem()
    
    print("=== MIRRALISM ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’è‡ªå·±æ¡ç‚¹ã‚·ã‚¹ãƒ†ãƒ  ===")
    print("ä½¿ã„æ–¹:")
    print("1. record_new_evaluation() ã§æ–°ã—ã„è©•ä¾¡è¨˜éŒ²")
    print("2. show_progress() ã§é€²æ—ç¢ºèª")
    print()
    
    # ç¾åœ¨ã®çŠ¶æ³è¡¨ç¤º
    system.show_progress() 