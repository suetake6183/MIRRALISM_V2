#!/usr/bin/env python3
"""
MIRRALISM æ­£ã—ã„è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
æœ«æ­¦ã•ã‚“ã®æŒ‡æ‘˜ã‚’å—ã‘ãŸä¿®æ­£ç‰ˆ

Author: MIRRALISM Technical Team
Version: 2.0 (ä¿®æ­£ç‰ˆ)
Created: 2025-06-10
"""

import json
from datetime import datetime, timezone
from pathlib import Path


class CorrectTestSystem:
    """æ­£ã—ã„è©•ä¾¡åé›†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.project_root = Path(__file__).parent.parent.parent
        self.results_file = self.project_root / "Data" / "correct_evaluation_results.json"
        self.results_file.parent.mkdir(parents=True, exist_ok=True)
        
        # æ—¢å­˜çµæœèª­ã¿è¾¼ã¿
        self.results = self._load_results()

    def _load_results(self):
        """æ—¢å­˜ã®è©•ä¾¡çµæœèª­ã¿è¾¼ã¿"""
        if self.results_file.exists():
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {"evaluations": []}
        return {"evaluations": []}

    def _save_results(self):
        """è©•ä¾¡çµæœä¿å­˜"""
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

    def record_evaluation(self, score, comment=None):
        """è©•ä¾¡ã‚’æ­£ã—ãè¨˜éŒ²"""
        evaluation_number = len(self.results["evaluations"]) + 1
        
        evaluation = {
            "evaluation_number": evaluation_number,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "score": score,
            "comment": comment if comment else None,
            "question": "é»’æ¾¤å·¥å‹™åº—ã«ã¤ã„ã¦æ•™ãˆã¦"
        }
        
        self.results["evaluations"].append(evaluation)
        self._save_results()
        
        print("âœ… è©•ä¾¡{}è¨˜éŒ²å®Œäº†: {}ç‚¹".format(evaluation_number, score))
        
        if comment:
            print("   ã‚³ãƒ¡ãƒ³ãƒˆ: {}".format(comment))
        else:
            print("   ã‚³ãƒ¡ãƒ³ãƒˆ: ãªã—")
        
        return evaluation

    def show_progress(self):
        """ç¾åœ¨ã®é€²æ—è¡¨ç¤º"""
        total_evaluations = len(self.results["evaluations"])
        
        if total_evaluations == 0:
            print("ğŸ“Š ã¾ã è©•ä¾¡ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        scores = [e["score"] for e in self.results["evaluations"]]
        average = sum(scores) / len(scores)
        
        print("\nğŸ“Š ç¾åœ¨ã®é€²æ—:")
        print("è©•ä¾¡å›æ•°: {}/10å›".format(total_evaluations))
        print("ç¾åœ¨ã®å¹³å‡: {:.2f}ç‚¹".format(average))
        print("ç†è§£ç²¾åº¦: {:.1f}%".format((average / 5.0) * 100))
        
        print("\nè©³ç´°:")
        for evaluation in self.results["evaluations"]:
            if evaluation["comment"]:
                comment_text = " - {}".format(evaluation["comment"])
            else:
                comment_text = ""
            
            print("  è©•ä¾¡{}: {}ç‚¹{}".format(
                evaluation["evaluation_number"], 
                evaluation["score"],
                comment_text
            ))

    def interactive_evaluation(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªè©•ä¾¡åé›†"""
        current_count = len(self.results["evaluations"]) + 1
        
        print("\n--- è©•ä¾¡ {}/10 ---".format(current_count))
        print("è³ªå•: ã€Œé»’æ¾¤å·¥å‹™åº—ã«ã¤ã„ã¦æ•™ãˆã¦ã€")
        print("(ã“ã®è³ªå•ã¸ã®ç§ã®å›ç­”ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„)")
        print()
        
        # è©•ä¾¡ç‚¹æ•°å…¥åŠ›
        while True:
            try:
                score_input = input("è©•ä¾¡{}: ä½•ç‚¹ã§ã™ã‹ï¼Ÿ (1-5): ".format(current_count))
                score = int(score_input)
                if 1 <= score <= 5:
                    break
                else:
                    print("âŒ 1ã‹ã‚‰5ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            except ValueError:
                print("âŒ æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        
        # ã‚³ãƒ¡ãƒ³ãƒˆå…¥åŠ›ï¼ˆä»»æ„ï¼‰
        print()
        comment = input("ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆä»»æ„ã€ãªã‘ã‚Œã°Enterï¼‰: ").strip()
        
        # ã‚³ãƒ¡ãƒ³ãƒˆãŒç©ºã®å ´åˆã¯Noneã«ã™ã‚‹
        if not comment:
            comment = None
        
        # è¨˜éŒ²
        evaluation = self.record_evaluation(score, comment)
        
        return evaluation

    def run_full_test(self):
        """å®Œå…¨ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("ğŸ¯ MIRRALISM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç†è§£ç²¾åº¦ãƒ†ã‚¹ãƒˆ")
        print("="*50)
        print()
        print("ã€ãƒ†ã‚¹ãƒˆæ‰‹é †ã€‘")
        print("1. ç§ã«ã€Œé»’æ¾¤å·¥å‹™åº—ã«ã¤ã„ã¦æ•™ãˆã¦ã€ã¨è³ªå•")
        print("2. ç§ã®å›ç­”ã‚’èª­ã‚“ã§è©•ä¾¡")
        print("3. ç‚¹æ•°ã¨ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰ã‚’å…¥åŠ›")
        print("4. ã“ã‚Œã‚’10å›ç¹°ã‚Šè¿”ã—")
        print()
        
        while len(self.results["evaluations"]) < 10:
            current_count = len(self.results["evaluations"]) + 1
            
            print("=" * 30)
            print("ãƒ†ã‚¹ãƒˆ {}/10å›ç›®".format(current_count))
            print("=" * 30)
            print()
            print("æœ«æ­¦ã•ã‚“ã€ç§ã«è³ªå•ã—ã¦ãã ã•ã„:")
            print("â†’ ã€Œé»’æ¾¤å·¥å‹™åº—ã«ã¤ã„ã¦æ•™ãˆã¦ã€")
            print()
            
            input("è³ªå•ã—ã¦å›ç­”ã‚’å—ã‘å–ã£ãŸã‚‰Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
            
            # è©•ä¾¡åé›†
            self.interactive_evaluation()
            
            # é€²æ—è¡¨ç¤º
            self.show_progress()
            
            if len(self.results["evaluations"]) < 10:
                print("\næ¬¡ã®ãƒ†ã‚¹ãƒˆã«é€²ã¿ã¾ã™...")
            else:
                print("\nğŸ‰ 10å›ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                self.calculate_final_result()

    def calculate_final_result(self):
        """æœ€çµ‚çµæœè¨ˆç®—"""
        if len(self.results["evaluations"]) < 10:
            print("âš ï¸  ã¾ã 10å›ã®è©•ä¾¡ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“")
            return
        
        scores = [e["score"] for e in self.results["evaluations"]]
        average_score = sum(scores) / len(scores)
        accuracy_percentage = (average_score / 5.0) * 100
        
        print("\n" + "="*60)
        print("ğŸ¯ æœ€çµ‚ãƒ†ã‚¹ãƒˆçµæœ")
        print("="*60)
        print("ç·è©•ä¾¡å›æ•°: {}å›".format(len(scores)))
        print("å¹³å‡ã‚¹ã‚³ã‚¢: {:.2f}ç‚¹ / 5ç‚¹".format(average_score))
        print("ç†è§£ç²¾åº¦: {:.1f}%".format(accuracy_percentage))
        print()
        
        # åˆ¤å®š
        if accuracy_percentage >= 90:
            print("ğŸŒŸ å„ªç§€! MIRRALISMã¯é»’æ¾¤å·¥å‹™åº—ã‚’éå¸¸ã«ã‚ˆãç†è§£ã—ã¦ã„ã¾ã™")
        elif accuracy_percentage >= 75:
            print("ğŸ‘ è‰¯å¥½! ã‚‚ã†å°‘ã—æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
        elif accuracy_percentage >= 60:
            print("âš ï¸  æ™®é€š: ã‹ãªã‚Šã®æ”¹å–„ãŒå¿…è¦ã§ã™")
        else:
            print("ğŸš¨ è¦æ”¹å–„: å¤§å¹…ãªç²¾åº¦å‘ä¸ŠãŒå¿…è¦ã§ã™")
        
        return accuracy_percentage


def manual_record():
    """æ‰‹å‹•è¨˜éŒ²ç”¨ã®é–¢æ•°"""
    system = CorrectTestSystem()
    
    print("ç¾åœ¨ã®çŠ¶æ³:")
    system.show_progress()
    print()
    
    print("æ–°ã—ã„è©•ä¾¡ã‚’è¨˜éŒ²ã—ã¾ã™:")
    evaluation = system.interactive_evaluation()
    
    print("\næ›´æ–°å¾Œã®çŠ¶æ³:")
    system.show_progress()
    
    return evaluation


if __name__ == "__main__":
    system = CorrectTestSystem()
    
    print("=== MIRRALISM æ­£ã—ã„è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  ===")
    print("ä½¿ã„æ–¹:")
    print("1. manual_record() ã§æ‰‹å‹•è©•ä¾¡è¨˜éŒ²")
    print("2. run_full_test() ã§å®Œå…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print()
    
    # ç¾åœ¨ã®çŠ¶æ³è¡¨ç¤º
    system.show_progress() 