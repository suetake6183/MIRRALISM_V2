#!/usr/bin/env python3
"""
MIRRALISM ç°¡å˜è©•ä¾¡è¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ 
æœ«æ­¦ã•ã‚“ã®è©•ä¾¡ã‚’è¨˜éŒ²ãƒ»ç®¡ç†

Author: MIRRALISM Technical Team  
Version: 1.0
Created: 2025-06-10
"""

import json
import os
from datetime import datetime, timezone
from pathlib import Path


class SimpleTestRecorder:
    """æœ«æ­¦ã•ã‚“ã®è©•ä¾¡ã‚’ç°¡å˜ã«è¨˜éŒ²ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.project_root = Path(__file__).parent.parent.parent
        self.results_file = self.project_root / "Data" / "suetake_evaluation_results.json"
        self.results_file.parent.mkdir(parents=True, exist_ok=True)
        
        # æ—¢å­˜çµæœèª­ã¿è¾¼ã¿
        self.results = self._load_results()

    def _load_results(self):
        """æ—¢å­˜ã®è©•ä¾¡çµæœèª­ã¿è¾¼ã¿"""
        if self.results_file.exists():
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {"evaluations": []}
        return {"evaluations": []}

    def _save_results(self):
        """è©•ä¾¡çµæœä¿å­˜"""
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

    def record_score(self, score, comment=""):
        """ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²"""
        evaluation = {
            "evaluation_number": len(self.results["evaluations"]) + 1,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "score": score,
            "comment": comment,
            "question": "é»’æ¾¤å·¥å‹™åº—ã«ã¤ã„ã¦æ•™ãˆã¦"
        }
        
        self.results["evaluations"].append(evaluation)
        self._save_results()
        
        print("âœ… è©•ä¾¡{}è¨˜éŒ²å®Œäº†: {}ç‚¹".format(
            evaluation["evaluation_number"], score
        ))
        
        if comment:
            print("   ã‚³ãƒ¡ãƒ³ãƒˆ: {}".format(comment))

    def show_progress(self):
        """ç¾åœ¨ã®é€²æ—è¡¨ç¤º"""
        total_evaluations = len(self.results["evaluations"])
        
        if total_evaluations == 0:
            print("ğŸ“Š ã¾ã è©•ä¾¡ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        scores = [e["score"] for e in self.results["evaluations"]]
        average = sum(scores) / len(scores)
        
        print("\nğŸ“Š ç¾åœ¨ã®é€²æ—:")
        print("è©•ä¾¡å›æ•°: {}/10å›".format(total_evaluations))
        print("ç¾åœ¨ã®å¹³å‡: {:.2f}ç‚¹".format(average))
        print("ç†è§£ç²¾åº¦: {:.1f}%".format((average / 5.0) * 100))
        
        print("\nè©³ç´°:")
        for evaluation in self.results["evaluations"]:
            comment_text = " - {}".format(evaluation["comment"]) if evaluation["comment"] else ""
            print("  è©•ä¾¡{}: {}ç‚¹{}".format(
                evaluation["evaluation_number"], 
                evaluation["score"],
                comment_text
            ))

    def calculate_final_result(self):
        """æœ€çµ‚çµæœè¨ˆç®—"""
        if len(self.results["evaluations"]) < 10:
            print("âš ï¸  ã¾ã 10å›ã®è©•ä¾¡ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“")
            return
        
        scores = [e["score"] for e in self.results["evaluations"]]
        average_score = sum(scores) / len(scores)
        accuracy_percentage = (average_score / 5.0) * 100
        
        # æœ€çµ‚çµæœä¿å­˜
        final_result = {
            "test_id": "client_understanding_{}".format(
                datetime.now().strftime('%Y%m%d_%H%M%S')
            ),
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "test_type": "client_understanding",
            "target_client": "é»’æ¾¤å·¥å‹™åº—",
            "total_evaluations": len(scores),
            "scores": scores,
            "average_score": average_score,
            "accuracy_percentage": accuracy_percentage,
            "evaluations": self.results["evaluations"]
        }
        
        # æœ€çµ‚çµæœãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        final_file = self.project_root / "Data" / "final_test_result.json"
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        print("\n" + "="*60)
        print("ğŸ¯ æœ€çµ‚ãƒ†ã‚¹ãƒˆçµæœ")
        print("="*60)
        print("ç·è©•ä¾¡å›æ•°: {}å›".format(len(scores)))
        print("å¹³å‡ã‚¹ã‚³ã‚¢: {:.2f}ç‚¹ / 5ç‚¹".format(average_score))
        print("ç†è§£ç²¾åº¦: {:.1f}%".format(accuracy_percentage))
        print()
        
        # åˆ¤å®š
        if accuracy_percentage >= 90:
            print("ğŸŒŸ å„ªç§€! MIRRALISMã¯é»’æ¾¤å·¥å‹™åº—ã‚’éå¸¸ã«ã‚ˆãç†è§£ã—ã¦ã„ã¾ã™")
        elif accuracy_percentage >= 75:
            print("ğŸ‘ è‰¯å¥½! ã‚‚ã†å°‘ã—æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
        elif accuracy_percentage >= 60:
            print("âš ï¸  æ™®é€š: ã‹ãªã‚Šã®æ”¹å–„ãŒå¿…è¦ã§ã™")
        else:
            print("ğŸš¨ è¦æ”¹å–„: å¤§å¹…ãªç²¾åº¦å‘ä¸ŠãŒå¿…è¦ã§ã™")
        
        print("\nâœ… æœ€çµ‚çµæœä¿å­˜: {}".format(final_file))
        
        return final_result


if __name__ == "__main__":
    recorder = SimpleTestRecorder()
    
    print("=== MIRRALISM è©•ä¾¡è¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ  ===")
    print("ä½¿ã„æ–¹:")
    print("1. record_score(ç‚¹æ•°, ã‚³ãƒ¡ãƒ³ãƒˆ) ã§è©•ä¾¡è¨˜éŒ²")
    print("2. show_progress() ã§é€²æ—ç¢ºèª")
    print("3. calculate_final_result() ã§æœ€çµ‚çµæœè¨ˆç®—")
    print()
    
    # ç¾åœ¨ã®çŠ¶æ³è¡¨ç¤º
    recorder.show_progress() 