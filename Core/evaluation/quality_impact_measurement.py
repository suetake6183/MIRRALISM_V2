#!/usr/bin/env python3
"""
MIRRALISM Evaluation Quality Impact Measurement
Purpose: Quantitative measurement of quality infrastructure impact on evaluation system
Design: Real-time metrics collection with historical comparison

Created: 2025-06-07
Version: 1.0.0
"""

import json
import time
import psutil
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import logging
import statistics
from collections import defaultdict


@dataclass
class PerformanceMetric:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    timestamp: datetime
    metric_type: str
    value: float
    unit: str
    context: Dict[str, any] = None


@dataclass
class QualityImpactReport:
    """å“è³ªå½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆ"""
    measurement_period: str
    baseline_period: str
    system_stability: Dict[str, float]
    evaluation_accuracy: Dict[str, float]
    development_efficiency: Dict[str, float]
    overall_improvement: float
    recommendations: List[str]


class QualityImpactMeasurement:
    """å“è³ªåŸºç›¤å½±éŸ¿æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.data_dir = self.project_root / "Data" / "quality_measurement"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self.db_path = self.data_dir / "quality_impact.db"
        self._init_database()
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æœŸé–“ï¼ˆå“è³ªåŸºç›¤å°å…¥å‰ï¼‰
        self.baseline_start = datetime(2025, 6, 1)
        self.baseline_end = datetime(2025, 6, 7, 19, 0)  # Phase 1é–‹å§‹å‰
        
        # æ¸¬å®šæœŸé–“ï¼ˆå“è³ªåŸºç›¤å°å…¥å¾Œï¼‰
        self.measurement_start = datetime(2025, 6, 7, 19, 0)  # Phase 1é–‹å§‹
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO)
        
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    metric_type TEXT NOT NULL,
                    value REAL NOT NULL,
                    unit TEXT NOT NULL,
                    context TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS stability_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    uptime_seconds REAL,
                    error_count INTEGER,
                    crash_count INTEGER,
                    recovery_time_seconds REAL,
                    memory_usage_mb REAL,
                    cpu_usage_percent REAL,
                    disk_usage_percent REAL
                )
            """)
            
            # è©•ä¾¡ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS accuracy_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    evaluation_id TEXT,
                    predicted_score REAL,
                    actual_score REAL,
                    accuracy_percentage REAL,
                    confidence_score REAL,
                    processing_time_ms REAL
                )
            """)
            
            # é–‹ç™ºåŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS efficiency_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    task_type TEXT,
                    completion_time_minutes REAL,
                    error_resolution_time_minutes REAL,
                    code_quality_score REAL,
                    automation_level_percent REAL
                )
            """)
            
            conn.commit()
            
    def collect_system_stability_metrics(self):
        """ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            disk = psutil.disk_usage('/')
            
            # ãƒ—ãƒ­ã‚»ã‚¹ç¨¼åƒæ™‚é–“ï¼ˆæ¦‚ç®—ï¼‰
            uptime = time.time() - psutil.boot_time()
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO stability_metrics 
                    (timestamp, uptime_seconds, error_count, crash_count, 
                     recovery_time_seconds, memory_usage_mb, cpu_usage_percent, disk_usage_percent)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    datetime.now().isoformat(),
                    uptime,
                    0,  # ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆï¼ˆåˆ¥é€”å®Ÿè£…ï¼‰
                    0,  # ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚«ã‚¦ãƒ³ãƒˆï¼ˆåˆ¥é€”å®Ÿè£…ï¼‰
                    0,  # å¾©æ—§æ™‚é–“ï¼ˆåˆ¥é€”å®Ÿè£…ï¼‰
                    memory.used / 1024 / 1024,  # MB
                    cpu_percent,
                    disk.percent
                ))
                conn.commit()
                
            logging.info(f"ğŸ“Š System stability metrics collected: CPU {cpu_percent}%, Memory {memory.percent}%")
            
        except Exception as e:
            logging.error(f"Failed to collect system metrics: {e}")
            
    def collect_evaluation_accuracy_metrics(self, evaluation_id: str, predicted_score: float, 
                                          actual_score: float, processing_time_ms: float):
        """è©•ä¾¡ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        accuracy = (1 - abs(predicted_score - actual_score) / 5.0) * 100  # 5ç‚¹æº€ç‚¹ã§ã®ç²¾åº¦
        confidence = min(100, accuracy + 10)  # ç°¡æ˜“ä¿¡é ¼åº¦è¨ˆç®—
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO accuracy_metrics 
                (timestamp, evaluation_id, predicted_score, actual_score, 
                 accuracy_percentage, confidence_score, processing_time_ms)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                evaluation_id,
                predicted_score,
                actual_score,
                accuracy,
                confidence,
                processing_time_ms
            ))
            conn.commit()
            
        logging.info(f"ğŸ¯ Evaluation accuracy recorded: {accuracy:.1f}% for evaluation {evaluation_id}")
        
    def collect_development_efficiency_metrics(self, task_type: str, completion_time_minutes: float,
                                             error_resolution_time_minutes: float = 0,
                                             code_quality_score: float = 100,
                                             automation_level: float = 100):
        """é–‹ç™ºåŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO efficiency_metrics 
                (timestamp, task_type, completion_time_minutes, error_resolution_time_minutes,
                 code_quality_score, automation_level_percent)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                task_type,
                completion_time_minutes,
                error_resolution_time_minutes,
                code_quality_score,
                automation_level
            ))
            conn.commit()
            
        logging.info(f"âš¡ Development efficiency recorded: {task_type} completed in {completion_time_minutes:.1f}min")
        
    def calculate_system_stability_impact(self) -> Dict[str, float]:
        """ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§å½±éŸ¿è¨ˆç®—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿
            cursor.execute("""
                SELECT AVG(memory_usage_mb), AVG(cpu_usage_percent), COUNT(*) as error_count
                FROM stability_metrics 
                WHERE timestamp BETWEEN ? AND ?
            """, (self.baseline_start.isoformat(), self.baseline_end.isoformat()))
            baseline_data = cursor.fetchone()
            
            # æ¸¬å®šæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿
            cursor.execute("""
                SELECT AVG(memory_usage_mb), AVG(cpu_usage_percent), COUNT(*) as error_count
                FROM stability_metrics 
                WHERE timestamp >= ?
            """, (self.measurement_start.isoformat(),))
            current_data = cursor.fetchone()
            
        if not baseline_data or not current_data or baseline_data[0] is None or current_data[0] is None:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã®å ´åˆã¯ã€V1ä»®å®šå€¤ã¨æ¯”è¼ƒ
            v1_baseline_memory = 2048.0  # MB (V1ã§ã®å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡)
            v1_baseline_cpu = 45.0       # % (V1ã§ã®å¹³å‡CPUä½¿ç”¨é‡)
            v1_baseline_errors = 10      # V1ã§ã®å¹³å‡ã‚¨ãƒ©ãƒ¼æ•°
            
            if current_data and current_data[0] is not None:
                memory_improvement = ((v1_baseline_memory - current_data[0]) / v1_baseline_memory) * 100
                cpu_improvement = ((v1_baseline_cpu - current_data[1]) / v1_baseline_cpu) * 100
                error_improvement = ((v1_baseline_errors - current_data[2]) / max(v1_baseline_errors, 1)) * 100
            else:
                return {"downtime_reduction": 0, "resource_efficiency": 0, "error_rate_reduction": 0}
        else:
            # æ”¹å–„ç‡è¨ˆç®—
            memory_improvement = ((baseline_data[0] - current_data[0]) / baseline_data[0]) * 100 if baseline_data[0] > 0 else 0
            cpu_improvement = ((baseline_data[1] - current_data[1]) / baseline_data[1]) * 100 if baseline_data[1] > 0 else 0
            error_improvement = ((baseline_data[2] - current_data[2]) / max(baseline_data[2], 1)) * 100
        
        return {
            "downtime_reduction": max(0, error_improvement),
            "resource_efficiency": max(0, (memory_improvement + cpu_improvement) / 2),
            "error_rate_reduction": max(0, error_improvement)
        }
        
    def calculate_evaluation_accuracy_impact(self) -> Dict[str, float]:
        """è©•ä¾¡ç²¾åº¦å½±éŸ¿è¨ˆç®—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ¸¬å®šæœŸé–“ã®ç²¾åº¦ãƒ‡ãƒ¼ã‚¿
            cursor.execute("""
                SELECT AVG(accuracy_percentage), AVG(confidence_score), AVG(processing_time_ms)
                FROM accuracy_metrics 
                WHERE timestamp >= ?
            """, (self.measurement_start.isoformat(),))
            current_data = cursor.fetchone()
            
        if not current_data or current_data[0] is None:
            return {"precision_improvement": 0, "confidence_increase": 0, "speed_improvement": 0}
            
        # V1ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆä»®å®šå€¤ï¼‰
        baseline_accuracy = 53.0  # V1ã®53%ç²¾åº¦
        baseline_confidence = 60.0
        baseline_processing_time = 1000.0  # 1ç§’
        
        accuracy_improvement = ((current_data[0] - baseline_accuracy) / baseline_accuracy) * 100 if current_data[0] else 0
        confidence_improvement = ((current_data[1] - baseline_confidence) / baseline_confidence) * 100 if current_data[1] else 0
        speed_improvement = ((baseline_processing_time - current_data[2]) / baseline_processing_time) * 100 if current_data[2] else 0
        
        return {
            "precision_improvement": max(0, accuracy_improvement),
            "confidence_increase": max(0, confidence_improvement),
            "speed_improvement": max(0, speed_improvement)
        }
        
    def calculate_development_efficiency_impact(self) -> Dict[str, float]:
        """é–‹ç™ºåŠ¹ç‡å½±éŸ¿è¨ˆç®—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ¸¬å®šæœŸé–“ã®åŠ¹ç‡ãƒ‡ãƒ¼ã‚¿
            cursor.execute("""
                SELECT AVG(completion_time_minutes), AVG(error_resolution_time_minutes), 
                       AVG(code_quality_score), AVG(automation_level_percent)
                FROM efficiency_metrics 
                WHERE timestamp >= ?
            """, (self.measurement_start.isoformat(),))
            current_data = cursor.fetchone()
            
        if not current_data or current_data[0] is None:
            return {"task_completion_speedup": 0, "error_resolution_speedup": 0, "quality_improvement": 0}
            
        # V1ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆä»®å®šå€¤ï¼‰
        baseline_completion_time = 120.0  # 2æ™‚é–“
        baseline_error_resolution = 60.0   # 1æ™‚é–“
        baseline_quality = 70.0            # 70%
        
        completion_improvement = ((baseline_completion_time - current_data[0]) / baseline_completion_time) * 100
        resolution_improvement = ((baseline_error_resolution - current_data[1]) / baseline_error_resolution) * 100 if current_data[1] and current_data[1] > 0 else 100
        quality_improvement = ((current_data[2] - baseline_quality) / baseline_quality) * 100 if current_data[2] else 0
        
        return {
            "task_completion_speedup": max(0, completion_improvement),
            "error_resolution_speedup": max(0, resolution_improvement),
            "quality_improvement": max(0, quality_improvement)
        }
        
    def generate_impact_report(self) -> QualityImpactReport:
        """ç·åˆå½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        # å„é ˜åŸŸã®å½±éŸ¿è¨ˆç®—
        stability_impact = self.calculate_system_stability_impact()
        accuracy_impact = self.calculate_evaluation_accuracy_impact()
        efficiency_impact = self.calculate_development_efficiency_impact()
        
        # ç·åˆæ”¹å–„åº¦è¨ˆç®—
        all_improvements = []
        all_improvements.extend(stability_impact.values())
        all_improvements.extend(accuracy_impact.values())
        all_improvements.extend(efficiency_impact.values())
        
        overall_improvement = statistics.mean(all_improvements) if all_improvements else 0
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_recommendations(
            stability_impact, accuracy_impact, efficiency_impact
        )
        
        return QualityImpactReport(
            measurement_period=f"{self.measurement_start.isoformat()} - {datetime.now().isoformat()}",
            baseline_period=f"{self.baseline_start.isoformat()} - {self.baseline_end.isoformat()}",
            system_stability=stability_impact,
            evaluation_accuracy=accuracy_impact,
            development_efficiency=efficiency_impact,
            overall_improvement=overall_improvement,
            recommendations=recommendations
        )
        
    def _generate_recommendations(self, stability: Dict, accuracy: Dict, efficiency: Dict) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # å®‰å®šæ€§ã®æ¨å¥¨
        if stability["error_rate_reduction"] < 50:
            recommendations.append("Consider implementing additional error prevention mechanisms")
            
        # ç²¾åº¦ã®æ¨å¥¨
        if accuracy["precision_improvement"] < 30:
            recommendations.append("Enhance PersonalityLearning training data quality")
            
        # åŠ¹ç‡ã®æ¨å¥¨
        if efficiency["task_completion_speedup"] < 40:
            recommendations.append("Expand automation coverage for development tasks")
            
        if not recommendations:
            recommendations.append("Quality infrastructure is performing excellently")
            
        return recommendations
        
    def start_continuous_monitoring(self, interval_seconds: int = 300):
        """ç¶™ç¶šçš„ç›£è¦–é–‹å§‹"""
        logging.info(f"ğŸ” Starting continuous quality impact monitoring (interval: {interval_seconds}s)")
        
        try:
            while True:
                self.collect_system_stability_metrics()
                time.sleep(interval_seconds)
        except KeyboardInterrupt:
            logging.info("ğŸ‘‹ Continuous monitoring stopped")
            
    def export_metrics_to_json(self, output_path: Optional[Path] = None) -> Path:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONå‡ºåŠ›"""
        if output_path is None:
            output_path = self.data_dir / f"quality_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
        # å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            metrics_data = {}
            
            # å®‰å®šæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            cursor.execute("SELECT * FROM stability_metrics ORDER BY timestamp DESC LIMIT 100")
            metrics_data["stability"] = [dict(zip([col[0] for col in cursor.description], row)) 
                                       for row in cursor.fetchall()]
            
            # ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            cursor.execute("SELECT * FROM accuracy_metrics ORDER BY timestamp DESC LIMIT 100")
            metrics_data["accuracy"] = [dict(zip([col[0] for col in cursor.description], row)) 
                                      for row in cursor.fetchall()]
            
            # åŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            cursor.execute("SELECT * FROM efficiency_metrics ORDER BY timestamp DESC LIMIT 100")
            metrics_data["efficiency"] = [dict(zip([col[0] for col in cursor.description], row)) 
                                        for row in cursor.fetchall()]
            
        # JSONå‡ºåŠ›
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metrics_data, f, indent=2, ensure_ascii=False, default=str)
            
        logging.info(f"ğŸ“ Metrics exported to: {output_path}")
        return output_path


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    measurement = QualityImpactMeasurement()
    
    # ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    measurement.collect_system_stability_metrics()
    
    # ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    measurement.collect_evaluation_accuracy_metrics(
        evaluation_id="test_001",
        predicted_score=4.2,
        actual_score=4.0,
        processing_time_ms=250
    )
    
    # ã‚µãƒ³ãƒ—ãƒ«é–‹ç™ºåŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    measurement.collect_development_efficiency_metrics(
        task_type="security_framework_implementation",
        completion_time_minutes=45,
        error_resolution_time_minutes=5,
        code_quality_score=95,
        automation_level=90
    )
    
    # å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = measurement.generate_impact_report()
    
    print("ğŸš€ MIRRALISM Quality Impact Report")
    print("=" * 50)
    print(f"ğŸ“… Measurement Period: {report.measurement_period}")
    print(f"ğŸ“Š Overall Improvement: {report.overall_improvement:.1f}%")
    print()
    print("ğŸ“ˆ System Stability:")
    for metric, value in report.system_stability.items():
        print(f"  {metric}: {value:.1f}%")
    print()
    print("ğŸ¯ Evaluation Accuracy:")
    for metric, value in report.evaluation_accuracy.items():
        print(f"  {metric}: {value:.1f}%")
    print()
    print("âš¡ Development Efficiency:")
    for metric, value in report.development_efficiency.items():
        print(f"  {metric}: {value:.1f}%")
    print()
    print("ğŸ’¡ Recommendations:")
    for rec in report.recommendations:
        print(f"  â€¢ {rec}")
        
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‡ºåŠ›
    output_file = measurement.export_metrics_to_json()
    print(f"\nğŸ“ Detailed metrics saved to: {output_file}")


if __name__ == "__main__":
    main()