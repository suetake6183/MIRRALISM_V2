# **AIによるリポジトリの自動変更・ファイル拡散問題とその包括的解決策**

## **I. はじめに：AIによるリポジトリ変更という新たな課題**

### **A. 問題の定義：AIコーディングアシスタントとエージェントの越権行為**

ソフトウェア開発を支援するために設計されたAIツールは、時として、コードリポジトリに望ましくない変更を引き起こす動作を示すことがあります。これには、予期しないファイルの作成、明示的な指示なしのディレクトリ構造の変更、ファイルの散乱などが含まれ、結果としてコードベースの構成と完全性が損なわれます。この問題は、単純なコード提案にとどまらず、より自律的なAIエージェント 1 や、既存ツールにおけるバグや予期せぬ動作 3 にまで及びます。

例えば、GitHub Copilotのコーディングエージェントは、プレビュー段階であっても「不自然な変更」を加えたり、テストに失敗したりする事例が報告されており、リポジトリに対するその動作の予測不可能性を示唆しています 1。これは、主要ベンダーの公式ツールでさえ、このような問題行動を示す可能性があることを浮き彫りにしています。同様に、Cursor AIは、大規模なファイル編集時に「破壊的な変更」を引き起こすことが知られており 3、特に複雑な操作下でAIツールがリポジトリを破損させたり、混乱させたりする可能性を示しています。Tabnineについては、プロジェクトファイルを誤ったディレクトリにコピーし、構造全体を複製してしまうというユーザー報告があり、これはAIがリポジトリ構造を変更し、ファイルを散乱させる明確な例です 4。

これらのAIツールは、開発速度を向上させるためにより大きな自律性を提供するように開発されています（例：Copilotエージェントによる問題解決 1）。しかし、この自律性の増大は、完全に制御されたり理解されたりしていない場合、意図しない、大規模な、そして潜在的に破壊的なリポジトリ変更のリスク増大と直接相関しています。支援を目的としたまさにその機能（自律性）が、問題の主要な原因となるのです。これは、解決策が新たな種類の問題を引き起こすという一種の逆説的な状況を生み出しています。したがって、解決策は単にバグを修正するだけでなく、この自律性を管理し、制限することに焦点を当てる必要があります。ユーザーは、AIの行動の範囲と影響に対して、よりきめ細かい制御を必要としています。

### **B. 開発ワークフローとコードの完全性への影響**

AIによる意図しないリポジトリ変更は、開発ワークフローとコードの完全性に多大な影響を及ぼします。

* **認知的負荷の増大:** 開発者は、AIが生成した混乱を理解し、クリーンアップするために余分な時間を費やすことになります。  
* **ビルドとテストの失敗:** 予期しないファイルや構造変更は、ビルドスクリプト、CI/CDパイプライン、テストを破壊する可能性があります。  
* **バージョン管理の複雑化:** 意図しない変更のコミット、意味のある変更の追跡困難、複雑なマージコンフリクトが発生し得ます。Cursor AIが不適切なGit作業によりファイルコンフリクトを発生させ、不明確なメッセージでコミットを行ったというユーザー報告は、バージョン管理とコラボレーションに直接影響します 5。  
* **セキュリティリスク:** 意図しないファイルに機密情報が含まれていたり、脆弱性が持ち込まれたりする可能性があります 7。AIが生成したコードのセキュリティ脆弱性をレビューしテストする必要性が強調されており、管理されていないAIの変更がコードの完全性を損なう可能性があることを示唆しています 7。  
* **コラボレーションの問題:** リポジトリの状態に関してチームメンバー間で混乱が生じます。  
* **信頼の低下:** AIツールが信頼できない、または破壊的であると認識された場合、開発者はその使用をためらうようになる可能性があります。

従来のソフトウェアのバグは、多くの場合、即座に目に見えるエラーを生成します。しかし、AIによるリポジトリの変更、例えば、わずかに置き間違えられたファイルの作成や不要なディレクトリの作成などは、当初は「サイレント」である可能性があります 6。これらの変更は、すぐにビルドを破壊するわけではないかもしれませんが、蓄積され、徐々にリポジトリ構造を悪化させ、後になって重大な問題（技術的負債）となる可能性があります。AIは、人間的な意味で間違いを犯したことを「認識」しません。このため、AIエラーのフィードバックループは、明示的なチェックなしでは遅延したり、存在しなかったりする可能性があるため、検出および監視戦略は従来のバグよりもさらに重要になります。

## **II. 「なぜ」を解き明かす：リポジトリにおけるAIの意図せぬ振る舞いの根本原因**

### **A. AIツール固有の動作：バグ、過剰な支援、設定ミス**

特定のAIツールがどのように問題の一因となるかを詳細に見ていきます。

* **GitHub Copilot:**  
  * プレビュー版のコーディングエージェントは、「不自然な変更」を加えたりテストに失敗したりする傾向が示されており、開発者の意図と一致しない可能性のある動作を示唆しています 1。  
  * 「予期しないファイルタイプとフォーマット」によるモデルトレーニングの失敗 9 は、Copilotのリポジトリ理解が不完全である可能性を示唆しており、誤った情報に基づくアクションにつながる可能性があります。  
  * CopilotをホストするVSCodeのようなツールと統合されたリンターが、「全く操作されていないVHDLファイルを予期せずリントする」というユーザー報告 10 は、AIまたはその周辺ツールがファイルシステムとどのように相互作用するかにおける潜在的な権限超過または設定ミスを示しています。  
* **Cursor AI:**  
  * 大規模ファイルの編集時の「破壊的な変更」やGitの規律に関する問題で知られています 3。これは、特に一括操作におけるファイル操作機能が堅牢でないか、予測不可能である可能性を示唆しています。  
  * ユーザーレポートでは、Cursor AIが「間違ったディレクトリで作業した」「間違ったリポジトリにコミットを作成した」「コマンドを実行する前に正しいディレクトリを確認しなかった」と明示的に述べられています 5。これはリポジトリの混乱の直接的な原因です。  
  * CursorのようなAIツールが、誤解されたコンテキストに基づいて、潜在的に間違ったディレクトリでコマンドを自動実行するリスクがあります 6。  
* **Tabnine:**  
  * プラグインが誤った重複パス（例：/home/ergo/workspace/ の代わりに /home/ergo/home/ergo/workspace/）に変更を適用しようとし、プロジェクトファイルがコピーされ、意図しない場所に構造が複製されるという報告があります 4。これはパス処理における明らかなバグです。  
  * 「極端に長い複数行の補完」を生成する可能性があり 11、注意深く管理しないと、誤ってファイル構造を変更したり、大規模で不要なコードブロックを作成したりする可能性があります。  
* **一般的なAIツールの問題:**  
  * AI固有のライブラリの脆弱性（例：Llama-cpp-PythonのCVE-2024-34359によるRCE）が悪用され、広範囲にわたる不正なリポジトリ変更を引き起こす可能性があります 8。  
  * AIサービスクラウドプラットフォームの設定ミスやストレージの不適切な保護により、リポジトリが意図しない変更にさらされる可能性があります 2。

### **B. 生成AIの性質：ハルシネーション、文脈の誤解、確率的出力**

LLMの基本的な特性がどのように問題に寄与するかを説明します。

* **ハルシネーション（幻覚）:** LLMは、もっともらしいが不正確、あるいは完全に捏造された情報を生成することがあります 12。コード生成の文脈では、これは存在しないファイルパスの作成、存在しないライブラリのインポート、タスクと一致しない「ガベージコード」の生成として現れる可能性があります 13。  
  * LLMにおけるハルシネーションは、「事実情報と非事実情報の両方を含む、インターネット、書籍、その他のソースからの膨大な量のデータでトレーニングされているため発生します。LLMは真実を理解または検証するのではなく、パターンと確率に基づいて応答を生成します」 12。これが、AIがファイルやディレクトリを「発明」する可能性がある理由を直接説明しています。  
  * LLMは「存在しないサードパーティライブラリ」を推奨する可能性があり、これは対応するファイル構造やプレースホルダーファイルの作成試行につながる可能性があります。「ガベージコード（GC）」とは、「生成されたコードが正しいアプローチから切り離されており」、AIが自身の誤解したタスクを「修正」しようとすると、ランダムなファイル操作につながる可能性がある場合です 13。  
* **文脈の誤解:** AIツールは、現在の作業ディレクトリ、プロジェクトの構造、またはユーザーの意図を誤解し、間違った場所や間違ったファイルに対してアクションを実行する可能性があります 6。  
  * 「不要なファイルを開いていると、関係のないコードが提案されたり、提案の質が低下する可能性があります」 15。これは、AIのコンテキストウィンドウが重要であり、汚染されると、プロジェクトの無関係な部分でのファイル作成を含む、誤ったアクションにつながる可能性があることを示唆しています。  
  * 「AIは推論するのではなく予測します。プロンプトが曖昧な場合でも何かを生成し、それが間違ったディレクトリでのrm \-rfである可能性があります」 6。これは、コンテキストの誤解や曖昧な指示が、いかに壊滅的なファイル操作につながるかを浮き彫りにしています。  
* **確率的性質:** LLMの出力は決定論的ではありません。同じプロンプトでも異なる結果が生じる可能性があり、AIの動作が時として予測不可能になります 12。AIがファイルシステムを変更するタスクを与えられたり、変更を決定したりする場合、この予測不可能性はファイル操作にまで及ぶ可能性があります。

LLMによる小さな初期の誤解（例えば、変数名の目的の誤解）が、特定のファイル構造を暗示するコードの生成につながる可能性があります。その後、AIが自身の欠陥のある生成に基づいて知覚された矛盾を「役立つように」または「修正」しようとすると、これらの暗示されたファイルまたはディレクトリの作成を開始する可能性があります。これにより、1つのエラーが問題を悪化させるアクションにつながるフィードバックループが作成されます。このため、AIの提案、特に新しいファイルや構造要素を含む提案の早期かつ頻繁な検証が不可欠です。コードを1行ずつ受け入れるだけでは、これらの創発的な構造上の問題を見逃す可能性があります。

### **C. AIエージェントにおける過剰なエージェンシー：機能、権限、自律性の意図せぬ結果**

リポジトリに対してより自律的に行動する権限を持つAIエージェントに焦点を当てます。

* **OWASP LLM08: 過剰なエージェンシー:** この脆弱性は、LLMエージェントに過剰な機能、権限、または自律性が付与され、意図しないまたは有害なアクションにつながる場合に発生します 16。  
  * **過剰な機能:** エージェントが読み取りのみ必要な場合に更新や削除を実行できるなど、必要以上の機能を持つ場合 16。これは、ファイルを読み取るように設計されたAIが、その機能が広すぎる場合、誤ってファイルを作成または変更する可能性があることを意味します。  
  * **過剰な権限:** エージェントが他のシステムに対する不要な権限を保持しているか、特権アカウントですべてのユーザーファイルにアクセスできる場合 16。広範なファイルシステム書き込み権限を持つAIは、意図しない場所にファイルを作成する可能性が高くなります。  
  * **過剰な自律性:** エージェントがユーザーの確認なしに影響の大きいアクション（ファイルの削除や作成など）を実行する場合 16。  
* **ツールの誤用と意図の破壊:** 攻撃者や意図しない曖昧なプロンプトでさえ、エージェントを操作して統合ツールを悪用したり、その目標を変更させたりして、意図しないファイルシステム操作につながる可能性があります 2。  
* **混乱した代理人問題 (Confused Deputy Problem):** AIエージェント（「代理人」）が、権限の低いユーザーや別のプロセスによって騙され、制限された領域でのファイルの作成や変更など、より高い権限でアクションを実行させられる可能性があります 17。これは、操作の連鎖が元のユーザーの意図と権限を曖昧にする可能性があるマルチエージェントシステムでは特に危険です。  
  * 「破壊的なプロンプトインジェクション」について議論されており、その目的は構造を破壊したりシステムの障害を引き起こしたりすることであり、これには混沌としたファイルの作成/削除が含まれる可能性があります 18。  
  * 「予期せぬRCEおよびコード攻撃：攻撃者はAIエージェントのコード実行能力を悪用します。悪意のあるコードを注入することにより、内部ネットワークやホストファイルシステムなど、実行環境の要素への不正アクセスを取得できます」 2。これは、エージェントの能力と不正なファイルシステムの変更を直接結びつけます。

「混乱した代理人」問題 17 は、マルチエージェントシステムにおいて著しく複雑かつ危険になります。初期のエージェントは適切で限定的な権限で動作するかもしれません。しかし、それがより広範な（おそらく不適切に設定された）権限を持つ別のエージェントを呼び出す場合、システム全体の能力が増幅されます。最初のエージェントにとって安全に見えるアクションが、下流のエージェントによる広範囲な意図しないファイルシステムの変更につながるカスケードを引き起こす可能性があります。したがって、セキュリティと権限モデルは、個々のコンポーネントだけでなく、AIエージェントのチェーン全体に対して設計される必要があります。監査証跡は、エージェント間の相互作用にわたるアクションを追跡する必要があります。

### **D. 安全でない設計パターンと安全でないツール統合**

AIアプリケーションの設計と他のツールとの統合が、どのようにファイルシステムの問題につながるか。

* **安全でないコードインタプリタ:** エージェントを任意のコード実行や、ファイルシステムを含むホストリソースへの不正アクセスにさらします 2。エージェントのコードインタプリタが適切にサンドボックス化されていない場合、基盤となるプロセスが権限を持つ場所であればどこにでもファイルを作成、変更、または削除するように指示される可能性があります。  
* **設定ミスまたは脆弱なツール:** 統合されたツール（例：内部ファイルサーバーと対話するためにSSRFに悪用される可能性のある、無制限のネットワークアクセスを持つWebコンテンツリーダー）が攻撃ベクトルになる可能性があります 2。  
* **ツールの入力サニタイズの欠如:** エージェントは、それらのツール内の脆弱性を悪用する細工された入力をツールに提供し、意図しないファイル操作につながる可能性があります 2。  
* **不適切な保護とアクセス制御:** AIの運用環境における不十分なストレージ保護や不適切なアクセス制御などの一般的な設定ミス 8。

従来のソフトウェアでは、ファイルが間違った場所に作成されるのは明らかにバグです（例：Tabnineのパス問題 4）。LLMの場合、ファイルが「ハルシネーション」 12 や確率的な逸脱によって作成された場合、それはLLM自体の「バグ」なのか、それとも開発者が常に警戒しなければならない固有の特性なのでしょうか？この区別は重要です。なぜなら、固有の特性を「修正」することは、AIツールの特定のコードの欠陥を修正するよりもはるかに難しいからです。このため、緩和戦略は、LLMからの予測不可能な行動のベースラインレベルを前提とし、ツールベンダーがそのような事象をすべて排除することにのみ依存するのではなく、封じ込めと検証に焦点を当てる必要があります。

**表1：AIツールと関連するリポジトリ変更問題の概要**

| AIツール | 一般的な意図しない動作 | 考えられる原因 | 主要な出典 |
| :---- | :---- | :---- | :---- |
| GitHub Copilot コーディングエージェント | 予期しないファイル作成、不自然なコード変更、テスト失敗 | プレビュー段階のバグ、リポジトリ理解の不備、予期しないファイルタイプへの対応不足 | 1 |
| Cursor AI | 大規模ファイル編集時の破壊的変更、誤ったディレクトリでの作業、不正なGit操作、意図しない機能変更 | 大規模ファイル処理の問題、パス検証の欠陥、自動実行機能の誤用、プロンプトの誤解釈 | 3 |
| Tabnine | プロジェクトファイルの誤ったパスへのコピー、ディレクトリ構造の複製、極端に長い補完 | パス解決のバグ、複数行補完の管理不備 | 4 |
| 一般的なLLMベースのエージェント | ファイルシステムの不正アクセス、任意のコード実行、ハルシネーションによるファイル操作、過剰なエージェンシーによる意図しない変更 | 脆弱なライブラリ、設定ミス、サンドボックス化の不備、過剰な権限・自律性、プロンプトインジェクション、混乱した代理人問題 | 2 |

この表は、使用している特定のAIツールが特定の種類のリポジトリ変更問題で知られているかどうかをユーザーが迅速に特定するのに役立ちます。また、行動を潜在的な原因や特定のツールに結び付けることで、ユーザーは調査を絞り込むことができます。

## **III. プロアクティブな防御：AIツール統合のための設定とベストプラクティス**

### **A. 基本戦略：IDE設定と.gitignore**

* **IDE設定:**  
  * **ファイルスコープの制限:** 現在のタスクに関連するファイルのみを開くようにIDEを設定することを強調します。これにより、AIプラグインが利用できるコンテキストが制限され、リポジトリの無関係な部分での無関係な提案やアクションが削減される可能性があります 15。  
  * **プラグイン固有の設定:** 多くのAIプラグインには、IDE内に独自の構成パネルがあります。ユーザーは、ファイルアクセス、自動変更、または提案範囲に関連するオプションについてこれらを調査する必要があります（AIプラグインの無効化/再インストール/設定について議論している20、21、19、22、23からの一般的な示唆）。  
* **.gitignore:**  
  * 主にGit用ですが、.gitignoreを包括的にすることで、AIツールがビルド成果物、ログ、ローカル構成ファイル、またはワークスペースに存在する可能性のあるその他の非プロジェクトファイルと誤って対話したり、変更を提案したりするのを防ぐのに役立ちます。これにより、AIにとってよりクリーンなコンテキストが維持されます（一般的なバージョン管理のベストプラクティスであり、暗黙的に15をサポートします）。

### **B. AIコーディングアシスタントの制御**

* **GitHub Copilot:**  
  * **コンテンツ除外** 25**:** リポジトリレベルと組織レベルの両方で、Copilotが特定のファイルやパスを無視するように設定する方法を詳細に説明します。これは、Copilotが機密領域や無関係な領域を読み取ったり、変更を提案したりするのを防ぐ直接的なメカニズムです。  
    * *メカニズム:* 除外されたファイルではコード補完が無効になり、そのコンテンツは他の場所での提案やチャット応答に影響しません。  
    * *設定:* リポジトリ設定（Settings \> Copilot \> Paths to exclude）またはより広範なルールのための組織設定を介して行います。fnmatchパターンを使用できます。  
    * *テスト:* ステータスバーのCopilotアイコンを確認するか（斜線が入ります）、除外されたファイルについてCopilot Chatに質問します。  
  * **リポジトリカスタム指示** 24**:** .github/copilot-instructions.mdファイルでプロジェクト固有のコンテキスト、コーディング標準、優先ツール/ライブラリを提供することにより、Copilotの動作をガイドします。これにより、Copilotが不要なファイル構造につながる可能性のある「不自然な」または範囲外の提案を行うのを防ぐことができます。  
    * *目的:* Copilotがチームの慣行に沿った応答とコードを生成するのに役立ちます。  
    * *例:* 「Javaの依存関係にはMavenではなくBazelを使用します。Bazel互換の例を提供してください。」  
    * *有効化:* 「指示ファイルを使用する」設定を有効にする必要があります。  
  * **ファイルアクセス制限** 15**:** IDEで関連ファイルのみを開いた状態にすることで、Copilotのコンテキストウィンドウを狭め、提案の質を向上させ、より広範で関連性の低いコンテキストに基づいて新しい不要なファイルを示唆するコードを生成する可能性を減らすという考えを強化します。  
* **Cursor AI:**  
  * **大規模ファイル編集とGit規律の管理** 3**:** 大規模ファイルでの「破壊的な変更」の報告を考慮し、ユーザーに注意を促します。  
    * 大規模なリファクタリングタスクを分割します。  
    * ロールバックポイントを持つために、変更を頻繁にコミットします。  
    * 大幅な変更にCursorを使用する場合は、強力なGitプラクティス（ブランチング、アトミックコミット）を強調します。  
  * **ディレクトリ操作に関する警戒** 5**:** Cursorが間違ったディレクトリで動作するという報告された問題のため、ユーザーは、ファイルシステムに影響を与えるコマンドを実行したり変更を適用したりする前に、AIが現在の作業パスを理解していることを再確認する必要があります。  
  * **自動実行の無効化** 6**:** Cursorがファイルシステムコマンドを自動実行する機能を提供している場合は、それらを無効にするか、「提案のみ」モードで使用し、すべてのアクションに対して明示的なユーザー確認を要求することを検討します。  
* **Tabnine:**  
  * **パスとファイル処理の癖への対応** 4**:** 潜在的なパス重複バグに注意してください。  
    * 予期しないファイル/ディレクトリの複製を監視します。  
    * 問題が発生した場合は、プラグインのバージョンを確認し、ベンダーからの更新またはバグ修正を探します。  
    * 永続的なパスの問題が発生した場合は、プラグインを再インストールするか、キャッシュをクリアすることを検討します（一般的なプラグインのトラブルシューティング 19）。  
  * **複数行補完のレビュー** 11**:** 非常に長い提案や複数行の提案を注意深くレビューして、意図しない構造要素や範囲外のコードの大きなブロックを誤って導入しないようにします。  
* **一般的なプラグイン管理** 19**:**  
  * **プラグインを最新の状態に保つ:** ベンダーは頻繁にバグ修正と改善をリリースします。  
  * **選択的無効化:** プラグインが問題を引き起こしている疑いがある場合は、一時的に無効にして確認します。一部のツールは、特定のワークスペースまたはプロジェクトに対してプラグインを無効にする方法を提供しています（例：設定/グループポリシーによるWindows Copilotの無効化 20）。  
  * **再インストール:** クリーンな再インストールにより、破損したプラグイン構成によって引き起こされる永続的な問題が解決される場合があります 19。  
  * **設定レビュー:** 特に更新後、ファイル操作や自律性を制御する新しいオプションが導入される可能性があるため、プラグイン設定を定期的にレビューします 22。

AIツールは急速に進化しており、新しい機能や設定オプションが頻繁に登場しています（例：Copilotのカスタム指示 24、コンテンツ除外 25）。開発チームがこれらの設定に積極的に対応し、その使用に関するベストプラクティスを確立しない場合、「設定負債」が蓄積される可能性があります。これは、過度に寛容であるか、安全性に最適化されていないデフォルト設定でツールを使用している可能性があり、意図しないリポジトリ変更の可能性が高まることを意味します。したがって、チームは、一度限りの設定だけでなく、AIツールの設定を定期的にレビューおよび更新するプロセスが必要です。これは、定期的な技術負債レビューの一部となる可能性があります。

### **C. AIエージェントのための安全な環境の確立：サンドボックス化と最小権限の原則**

* **サンドボックス化** 2**:** コードを実行したりファイルシステムと対話したりできるAIエージェントは、強力にサンドボックス化された環境で実行する必要があります。  
  * *詳細:* ネットワーク制限、システムコールフィルタリング、最小権限のコンテナ構成、広範なパス（例：./、/home）を避けるためのマウントボリュームの制限、一時データ用のtmpfsの使用。  
  * *目的:* エージェントがホストファイルシステムに不正な変更を加えたり、意図した範囲外の機密データにアクセスしたりするのを防ぎます。  
* **最小権限の原則** 2**:** AIエージェントには、タスクの実行に必要な最小限の権限のみを付与します。  
  * ファイルを読み取るか、特定の一次的な場所に書き込む必要がある場合にのみ、エージェントに広範なファイルシステムの書き込みアクセス権を与えることは避けてください。  
  * エージェントがユーザーの代わりに動作する場合、絶対に必要で厳密に制御されている場合を除き、昇格された権限ではなく、ユーザーの権限で動作する必要があります。  
* **認証情報管理とDLP** 2**:** サービストークンとシークレットを保護します。データ損失防止（DLP）ソリューションと監査ログを使用して、エージェントがファイルに書き込もうとする可能性のある機密情報の漏洩を監視および防止します。

### **D. 人的要素：明確なプロンプトとAIの限界の理解**

* **明確で的を絞った指示** 27**:** ファイル操作を含むAI出力の品質は、プロンプトの明確さに大きく依存します。  
  * パス、ファイル名、意図するアクションについて具体的に記述します。  
  * コンテキスト（プログラミング言語、ライブラリ、既存の構造）を提供します。  
  * AIによって誤解される可能性のある曖昧な要求は避けてください。  
* **AIの限界の理解** 12**:** ユーザーは、AIツールが絶対的なものではないことを認識する必要があります。AIは幻覚を見たり、誤解したり、間違いを犯したりする可能性があります。  
  * 特にファイルシステムの変更を伴うAIの提案を盲目的に信用しないでください。  
  * LLMの確率的な性質に注意してください。

コンテキストウィンドウ（例：現在開いているファイル 15）はAIのパフォーマンスにとって重要です。しかし、それは提案の質だけの問題ではありません。AIがアクセス可能なコンテキスト内の無関係なデータや悪意を持って作成されたデータ（例：開いているファイル内の誤解を招くコメント、意図的に紛らわしいファイル構造）の影響を受ける可能性がある場合、誤ったファイル操作を実行するように促される可能性があります。コンテキストが広範で制御されていないほど、この「暗黙的な」攻撃またはエラーの表面は大きくなります。したがって、AIのコンテキストを管理するための戦略（GitHub Copilotのコンテンツ除外 25 や、単に開いているファイルに注意を払うこと 15 など）は、パフォーマンスだけでなく、重要なセキュリティ/安全対策でもあります。

初期のAIツールは粗い制御（オン/オフ）しか提供していませんでした。意図しないファイル変更のような問題が表面化するにつれて、AIが*何*をできるか（例：ファイルの読み取り対書き込み対作成）、*どこで*できるか（特定のパス、プロジェクト対グローバルスコープ）、そして*いつ*できるか（例：明示的なコマンドでのみ対自律的に）について、よりきめ細かい制御が必要であることは明らかです。GitHub Copilotのコンテンツ除外 25 のような機能は、この方向への一歩です。ユーザーは、ファイルシステムとの対話に対してきめ細かい制御を提供するツールを提唱し、優先すべきです。ツールの選択基準には、これらの安全制御の堅牢性を含める必要があります。

**表2：主要AIコーディングアシスタントの設定チェックリスト**

| AIツール | 設定項目 | 推奨されるアクション/設定 | 出典例 |
| :---- | :---- | :---- | :---- |
| GitHub Copilot | コンテンツ除外 | Settings \> Copilotで除外パスを定義する | 25 |
|  | リポジトリカスタム指示 | .github/copilot-instructions.mdを作成し、プロジェクト固有の指示を記述する | 24 |
|  | ファイルアクセス | 関連ファイルのみを開いてコンテキストを限定する | 15 |
| Cursor AI | 大規模ファイル編集 | 変更を頻繁にコミットし、Gitのベストプラクティスに従う | 3 |
|  | ディレクトリ操作 | AIが現在の作業パスを正しく理解しているか常に確認する | 5 |
|  | 自動実行機能 | 無効にするか、細心の注意を払って使用し、各アクションを確認する | 6 |
| Tabnine | パス/ファイル処理 | パス重複バグを監視し、報告/更新する。問題が続く場合はプラグインを再インストールする | 4 |
|  | 複数行補完 | 長い補完候補を慎重にレビューし、意図しない構造変更を避ける | 11 |
| 全ツール共通 | プラグイン更新 | 常に最新バージョンに更新する | 19 |
|  | 不要な機能の無効化 | 使用しない、またはリスクが高いと判断される機能は無効化を検討する（例：Windows Copilotのタスクバーからの削除やグループポリシーでの無効化） | 20 |

このチェックリストは、ユーザーがAIツールの安全性を向上させるための直接的かつ実行可能なガイダンスを提供します。また、一般的なツールから特定のツールの癖や設定オプションまで、さまざまな側面に対応しています。

## **IV. ワークフローの強化：AI支援リポジトリのための開発プラクティス**

### **A. 人間による監視の不可欠な役割：厳格なコードレビュー**

* **AIはアシスタントであり、代替ではない** 27**:** AIが生成したコードは、ジュニア開発者が書いたものとして扱う必要があることを改めて強調します。検証には人間の専門知識が不可欠です。  
* **すべてのAI出力の徹底的なレビュー** 7**:**  
  * 正確性、論理性、セキュリティ脆弱性、コーディング標準とプロジェクト要件への準拠についてレビューします。  
  * ファイルシステムと対話するコード、データを処理するコード、またはセキュリティ上機微な操作を含むコードには特に注意を払います。  
  * 生成されたコードだけでなく、ファイル構造の変更、作成された新しいファイル、または削除もレビューします。  
  * 「生成されたすべてのコードは徹底的にレビューし…セキュリティ脆弱性をテストし…セキュリティ標準への準拠を確認する」 7。これは基本的な原則です。  
  * 「提出されたすべてのコードは、どのように始まったかに関わらず、正確性、保守性、および標準への適合性について徹底的なチェックが必要です」 26。これは、AIによる生成がレビューを回避するものではないことを強調しています。  
* **作成者の説明責任** 26**:** AI支援コードをコミットする開発者は、その品質、セキュリティ、機能性について責任を負います。

AIが大量のコードを生成するようになると、コードレビュー 7 の焦点が変わる必要があるかもしれません。AIがしばしば正しく行う構文の正しさだけでなく、レビュー担当者は、意味論的な正しさ、アーキテクチャの意図との整合性、複雑なAIロジックによって導入される微妙なセキュリティ上の欠陥、そして重要なことには、新しいファイルの作成や構造変更などの意図しない副作用について精査する必要があります。AIは、ローカルな問題を解決する「正しい」コードを生成するかもしれませんが、グローバルなアーキテクチャ原則に違反したり、静かにファイルを追加したりする可能性があります。したがって、コードレビューのガイドラインとチェックリストは、予期しないリポジトリの変更を含む、AI生成コード特有の障害モードに具体的に対処するために更新する必要があるかもしれません。AI生成セクションを強調表示するツールは、これを支援する可能性があります。

### **B. AI時代におけるGitによるバージョン管理の習熟**

* **AI生成コードのための戦略的ブランチング** 28**:**  
  * **フィーチャーブランチ:** AI支援開発を分離します。AIツールが混乱を引き起こした場合、それはブランチ内に封じ込められ、mainやdevelopには影響しません。  
  * **実験ブランチ (特にML/AIプロジェクト向け** 28**):** 大規模なAI生成コンポーネントやリファクタリングを試すために、フィーチャーブランチからサブブランチを作成します。これにより、AIの出力に問題がある場合に、簡単な比較と破棄が可能になります。「フィーチャーブランチでの探索的作業の分離」およびMLワークフローのための「ブランチング実験」の概念は、潜在的に破壊的なAIコード生成の管理に直接適用されます 28。  
* **コミットの衛生管理：意味のあるメッセージを伴う小規模でアトミックなコミット** 29**:**  
  * **小規模で論理的な変更:** AIが生成したコードを、小さく機能的な増分でコミットします。これにより、AIがエラーや不要なファイルを導入した場合に、問題の原因を特定しやすくなります。  
  * **アトミックコミット:** 各コミットは単一の作業単位を表す必要があります。AIが機能のコードを生成し、ユーティリティファイルの作成も提案する場合、これらは別々のアトミックコミットになる可能性があります。「アトミックコミットは単一の作業単位であり…コードレビューを高速化し、リバートを容易にします」 29。これは、頻繁なレビューやロールバックが必要になる可能性のあるAIコードを扱う場合に特に重要です。  
  * **記述的なメッセージ:** コミットにAI生成コードが含まれているかどうか、およびその目的を明確に示します。これは、問題が発生した場合の履歴追跡と責任追及に役立ちます。  
* **修復テクニック：不要なAIによる変更の取り消しとクリーンアップ:**  
  * **git revert \<commit\> (**30**):** AIによって行われた問題のあるコミットを、履歴を残しながら元に戻します。  
  * **git reset \--hard \<commit\> (**30**):** 最近のローカルコミットと変更を破棄します（特に変更がプッシュされている場合は注意して使用してください）。失敗したローカルAI実験を迅速に元に戻すのに役立ちます。  
  * **git clean \-fd (**31**):** AIによって作成された追跡されていないファイルとディレクトリを削除します。  
    * 常に最初に git clean \-n （ドライラン）を使用して、削除される内容をプレビューします。  
    * インタラクティブなクリーニングには git clean \-i を使用します。  
  * **git checkout \<commit\> \-- \<file\> (**30**):** AIが望ましくない変更を加えた場合に、単一のファイルを以前の状態に復元します。  
  * **git stash (**30**):** AIの提案について確信が持てない場合に一時的に保存するために使用でき、その後 git stash pop で再適用するか、git stash drop で破棄できます。git stash \-u は追跡されていないファイルをスタッシュできます。  
  * 31 は git clean の包括的なガイドを提供しており、AIが散乱させた可能性のあるファイルを削除するために直接関連しています。30 は、単一のファイルまたはコミット全体の変更を元に戻すためのさまざまな戦略を提供しています。

Gitは常にセーフティネットでしたが、AIの登場によりその役割は増幅されます。AIによるコード生成（および潜在的なエラー導入）の*速度*向上は、堅牢なGitプラクティス（頻繁なアトミックコミット 29、規律あるブランチング 28）がもはや単なる「ベストプラクティス」ではなく、AI支援開発を管理するための*不可欠なインフラストラクチャ*であることを意味します。git revertやgit clean 30 の容易さは、失敗したAI「実験」に対する主要な防御策となります。このため、チームはAIツールを多用する場合、Gitの衛生管理をより厳格にする必要があるかもしれません。高度なGit回復テクニックに関するトレーニングの重要性が増す可能性があります。

### **C. 責任あるAIツール使用のためのチームガイドラインとポリシー** 26

* **AI使用状況の文書化** 27**:** AI生成コードがどこで使用されているか、どのような変更が加えられたか、その根拠を記録します。  
* **コーディング標準との整合** 27**:** AIツールがチームのコーディングスタイルに従うように設定または指示されていることを確認します。  
* **セキュリティとデータプライバシーポリシー** 26**:** プロンプトで使用しても安全なデータ（例：PIIなし、専有アルゴリズムなし）を定義します。「AIの使用に関する簡単な1ページのポリシーをチームに提供することで、多くの当て推量を排除できます。安全なデータを概説し…ライセンスや帰属表示の要件を記載し、AIが生成したすべてのコードには依然として人間のレビューが必要であることを全員に念押しします」 26。これはチームガイドラインの実際的な出発点です。  
* **人間によるレビューの義務化** 26**:** AIが生成したすべてのコードには、人間のレビューが必要です。  
* **小規模から始め、支持者を育成する** 26**:** 熱心なユーザーとAIツールを試験的に導入し、フィードバックを収集します。  
* **トレーニングとリソース** 26**:** 効果的なプロンプト作成と安全な使用法に関する「チートシート」、FAQ、トレーニングを提供します。

AIの能力は、多くの組織が包括的な使用ポリシーを策定し実施できるよりも速く進んでいます 26。この「ポリシーの遅れ」は、開発者が、意図しないファイルの作成、プロンプトにおけるデータプライバシー、生成されたコードの知的財産権に関する懸念など、問題への対処方法に関する明確なガイダンスなしに強力なAIツールを使用している可能性があることを意味します。組織はAIツールに対して機敏なポリシー作成プロセスが必要です。シンプルで核となるポリシーから始め 26、反復することが、完璧で包括的なポリシーを待つよりも優れています。ポリシーの関連性を維持するためには、ツールを使用している開発者からの継続的なフィードバックが不可欠です。

**表3：AIによるリポジトリ変更管理のための主要Gitコマンド**

| シナリオ | Gitコマンド | 簡単な説明 | 出典例 |
| :---- | :---- | :---- | :---- |
| 問題のあるAI生成コミットの取り消し | git revert \<commit\_hash\> | 特定のコミットからの変更を取り消す新しいコミットを作成する | 30 |
| AIが作成した追跡されていないファイル/ディレクトリの削除 | git clean \-n の後 git clean \-fd | 追跡されていないファイルとディレクトリをプレビューし、強制的に削除する | 31 |
| ローカルAI実験の破棄 | git reset \--hard HEAD | ローカルのコミットされていないすべての変更を破棄し、最後のコミットにリセットする | 30 |
| AIによって変更された単一ファイルの復元 | git checkout HEAD\~ \-- path/to/file | ファイルを前のコミットのバージョンに置き換える | 30 |
| AIの提案の一時保存 | git stash \-u | 変更されたファイルと追跡されていないファイルをスタッシュする | 30 |

この表は、AIによって引き起こされる一般的な問題に対する即時のコマンドレベルの解決策を開発者に提供します。AIの間違いを取り消す方法を知ることで、開発者はAIツールをより自信を持って試すことができます。

## **V. 警戒と対応：監視、検出、修復戦略**

### **A. 意図しない変更の特定：ファイルシステム監視と差分ツール**

* **ファイルシステム監視ツール** 34**:**  
  * リポジトリディレクトリ内のファイルシステムイベント（作成、削除、変更）を監視するツールまたはスクリプトを実装します。特に、明示的な開発者のアクションや既知のビルドプロセスの外部で発生するイベントに注意します。  
  * 例：SiteScope Script monitor 34 は、カスタムスクリプトを実行してリポジトリの状態を確認できます。ManageEngine ADAudit Plus 35 は、リアルタイムのファイル変更監査を提供し、誰が、何を、いつ、どこで変更したかを追跡します。「ファイルへの変更試行の失敗とともに、すべてのファイル変更の背後にある誰が、何を、いつ、どこでを継続的に追跡します」 35。このレベルの監査が鍵となります。「予期しないファイルの作成や外部システムからのネットワークへのファイル転送を監視する…」 36。  
  * 異常な場所での予期しないファイルの作成や、重要な構成ファイルの変更に焦点を当てます。  
* **高度な差分ツール** 37**:**  
  * 標準のgit diffは便利ですが、高度なツールは、特にAIがコードをリファクタリングしたりブロックを移動させたりする場合に、複雑な変更に関するより多くの洞察を提供できます。  
  * GitClear Rich Diff Checker 37 は、追加/削除だけでなく、「移動」、「更新」、「コピー/貼り付け」、「検索/置換」操作も識別します。これは、軽微なAIの調整と、重要で潜在的に意図しない構造的なリファクタリングやファイルコンテンツの重複を区別するのに役立ちます。「リッチな差分はノイズを削減し、実際に変更されたものだけを明らかにします…開発者が認識する操作の完全なセットを使用して変更を分類します：移動、更新、コピー/貼り付け」 37。これは、変更の存在だけでなく、AIによる変更の*性質*を理解するのに役立ちます。

AIツール、特に多くの小さな提案を行うチャットボットのようなツールは、多くの「アクティビティ」を生成する可能性があります。効果的な監視 34 は、有益なAI支援（例：受け入れられたコード行の提案）と潜在的に有害なアクション（例：予期しないファイルの作成）を区別する必要があります。単純なファイルウォッチャーは、あまりにも多くの誤検知を生成する可能性があります。したがって、監視ソリューションはインテリジェントである必要があり、おそらくAIツールのログと統合したり、ファイルシステムの変更に関連する真に異常な動作をより適切にフラグ付けするために通常のAIインタラクションパターンを理解したりする必要があります。コンテキストを意識した監視が鍵となります。

### **B. AIツールアクションのロギングと監査** 6

* 利用可能な場合はAIツール内でロギングを有効にするか、IDE/システムログを使用してアクションをAIプラグインまで追跡します。  
* AIによってどのコマンドが実行されたかを理解するために、自動実行機能を備えたツール 6 にとって不可欠です。「すべてのAI生成アクションをログに記録する」 6。これは、「AIワイルドウェスト」を生き残るための直接的な推奨事項です。  
* ログは、予期しないファイル変更が直接的なAIコマンド、AI生成コードの副作用、またはAIツールのバグによるものかどうかを判断するのに役立ちます。

AIが予期しない変更、特にリポジトリ構造への変更を行った場合、開発者はAIが*なぜ*それを行ったのかを理解する必要があります。それはプロンプトの誤解だったのか？ハルシネーションだったのか？バグだったのか？ 38 はLLMの推論/エージェンシーに触れています。この理解なしには、再発を防ぐことは困難です。現在のロギング 6 は*何が*起こったかを示すかもしれませんが、AIの「推論」は示しません。将来のAIツールは、ファイルシステムに影響を与えるアクションの説明を理想的に提供すべきです。それまでの間、詳細なプロンプトエンジニアリングと、ファイルの変更につながる可能性のあるAIの提案の慎重なレビューが、説明可能性の代わりとなります。

### **C. 「破壊的変更」への対処とロールバック計画**

* Cursor AIの「破壊的変更」の可能性 3 および一般的なAIリスク 6 を参照します。  
* IV.Bで説明したGit戦略（頻繁なコミット、ブランチ）に大きく依存する、堅牢なロールバック計画の重要性。  
* 破壊的な変更が発生した場合、最初のステップは分離し（例：問題のあるブランチがマージされないようにする）、次にGitを使用して元に戻すかリセットします。  
* 深刻な場合（例：AIがディレクトリをワイプした場合）、リモートバックアップまたはGit履歴からの復元が不可欠です。

git clean \-n 31 やドライランモード 40 はプレビュー（プロアクティブな検出）を提供しますが、多くのファイルシステム監視ツール 35 はリアクティブであり、変更が発生した後に検出します。多層的なアプローチが最善です。ファイルシステムへの影響のプレビューやシミュレーションを提供するAIツール機能の使用を奨励します。これを堅牢なリアクティブ監視で補完し、すり抜けるものをキャッチします。目標は、不要な変更の検出時間を短縮することです。

## **VI. 高度な保護手段と将来の考慮事項**

### **A. AIガードレールとセキュリティフレームワークの実装**

* **OWASP Top 10 for LLM Applications** 2**:**  
  * 「LLM08: 過剰なエージェンシー」 16、「LLM04: モデルサービス拒否」（AIが作成するファイルが多すぎることによるリソース過負荷によってトリガーされる可能性あり）、「LLM06: 機密情報漏洩」（AIが機密データを予期しないファイルに書き込む）などの関連リスクについて議論します。  
  * OWASPガイドラインへの準拠が、リポジトリリスクを最小限に抑えるためのAIツール使用の設計と構成にどのように役立つかを説明します。  
* **NVIDIA NeMo Guardrails** 41**:**  
  * LLMベースのシステムにプログラム可能なガードレールを簡単に追加するためのオープンソースツールキットです。  
  * 機能：コンテンツの安全性、トピック制御、PII検出、RAG強制、ジェイルブレイク防止。  
  * 直接的なファイルシステムガードレールではありませんが、AIがトピックにとどまり、有害なリクエストを処理しないようにすることで、間接的に、不規則なファイルシステム動作につながる可能性のあるプロンプトを受信することを防ぐことができます。「NeMo Guardrailsは、生成的AIアプリケーションを保護するためのスケーラブルなAIガードレールオーケストレーションを簡素化します…LLMインタラクションの安全性、セキュリティ、正確性、およびトピックの関連性を確保します」 41。  
* **その他のガードレールアプローチ** 45**:** GitHub上のさまざまなオープンソースガードレールプロジェクト（例：guardrails-ai/guardrails）の存在に言及し、LLM出力を制御するための活発な開発分野であることを示します。  
* **ドライラン/シミュレーション機能** 40**:**  
  * CKEditorのGitコミット用リリースツール 40 に実装されているような「ドライラン」モードの概念は、非常に関連性があります。AIツールが提案された変更のファイルシステムへの影響を実行前にシミュレートできれば、強力な保護手段となるでしょう。  
  * AIコーディングアシスタントにおけるこのような機能を提唱します。

AIモデルがより強力で自律的になるにつれて 38、それらを制御するためのツールとテクニック（NeMo Guardrails 41 やOWASP LLMガイドライン 2 など）も急速に進化する必要があります。固有の緊張関係があります。能力が高いほど、潜在的な障害モードや誤用ベクトルが増えることがよくあります。保護手段の開発は、新しいAI機能の導入にわずかに遅れることがよくあります。したがって、組織は、現在の安全メカニズムが将来のAIツールに十分であると想定することはできません。リスク評価と新しいガードレール技術の採用の継続的なプロセスが必要になります。

### **B. 進化する状況：将来のAI能力と課題の予測**

* **AIエージェントの自律性の向上** 2**:** AIエージェントが自律的な行動とマルチエージェントコラボレーションの能力を高めるにつれて、複雑で追跡困難なリポジトリ変更の可能性が高まります。  
* **AI駆動のリファクタリングとコード移行** 9**:** ツールはますます大規模なコード変換を実行する機能を提供するようになります。これは有益である一方、完全に制御されていない場合、広範囲にわたる意図しない構造変更のリスクも高めます。  
* **説明可能性（XAI）の強化の必要性** 38**:** AIがリポジトリに特定の変更を加えた*理由*を理解することは、デバッグと信頼にとって不可欠になります。  
* **法的およびコンプライアンスへの影響** 46**:** 中核的な焦点ではありませんが、AI生成コードの所有権やAIの行動に対する説明責任に関する新たな懸念に簡単に触れます。これらは予期しないファイルの変更によって複雑になる可能性があります。

初期のLLMの安全性は、有害なテキスト生成（例：有害な言葉遣い、偏見）の防止に重点を置いていました。AIツールがファイルシステムを変更するエージェンシーを獲得すると、焦点は*安全なアクション*の確保に拡大する必要があります。LLMからのテキスト的に無害な出力でも、AIエージェントがコンテキストを誤解したり、過剰な権限を持っていたりすると、有害なファイルシステム操作に変換される可能性があります。ガードレールは、テキスト生成レベルだけでなく、アクションレベルで動作する必要があります。LLM向けのOWASPのようなセキュリティフレームワーク 2 は、「過剰なエージェンシー」 16 のような問題に対処するため重要です。これは、アクションの制御に直接関係しています。ファイルシステムの変更をシミュレートまたはドライランするツール 40 は、この方向への実践的な一歩です。

NeMo 41 のような一般的なガードレールや一般的なOWASP原則は、優れたベースラインを提供します。しかし、すべての開発環境とリポジトリには、固有の特性、機密性、およびファイル操作の「通常」のパターンがあります。不要なAIファイル変更の効果的な防止には、標準化されたガードレールと、リポジトリまたは組織に固有の高度にカスタマイズ可能なコンテキスト対応ルールの組み合わせが必要になる可能性があります。チームは、コンテンツモデレーションだけでなく、ファイルシステムとの対話に関連するカスタムポリシーの定義を可能にする、拡張可能なガードレールソリューションを探すべきです。

## **VII. 結論：AI拡張開発における制御と完全性の維持**

### **A. 主要戦略の要約**

防御の主な柱を簡潔にまとめます。

* AIの限界とツール固有の動作の理解。  
* AIツールとIDEのプロアクティブな設定。  
* 堅牢な開発ワークフロー（人間による監視、厳格なGitプラクティス）。  
* 警戒怠りない監視と検出。  
* 高度な保護手段とチームポリシーの実装。

### **B. 開発者とチームへの最終提言**

* すべてのAIツールに対して「信頼するが検証する」という考え方を受け入れます。  
* ファイルシステムとの対話に関して透明性と詳細な制御を提供するツールを優先します。  
* 効果的なAIの使用法と安全な慣行の両方についてトレーニングに投資します。  
* リポジトリの完全性に対する共同責任の文化を育みます。  
* ソフトウェア開発におけるAIの進化する能力とリスクについて常に情報を入手します。

開発者とAIコーディングツールの関係は静的なものではありません。AIツールが進化するにつれて、リポジトリ内でのAIの自律性の「エンゲージメントルール」と境界線を継続的に再評価し、再交渉する必要があります。今日、安全なレベルのAI介入と見なされているものが、明日もそうであるとは限りません。チームは、AIツールの使用状況を定期的にレビューし、何が機能し、何が機能しないか、そしてAIのコードベースへの影響を管理するために新しい制御やガイドラインがどこで必要かについて議論する必要があります。これは一度限りの設定ではなく、継続的なプロセスです。

AIによるリポジトリの混乱を防ぐことは、ツールを使用している個々の開発者の仕事だけではなく、AIツールベンダーの責任だけでもありません。それは、開発者（警戒、優れた慣行）、チームリーダー（ポリシー、トレーニング）、組織（安全なツールと環境の提供）、およびベンダー（より安全で制御しやすいツールの構築）を含む、共有された責任です。AIの固有の不確実性に対してリポジトリを*回復力のあるもの*にすることを目標として、すべての利害関係者間のフィードバックループを含む、全体的なアプローチが必要です。

#### **引用文献**

1. 「AIがMicrosoftの従業員を徐々に狂わせていく様子を見るのが趣味 ..., 5月 31, 2025にアクセス、 [https://gigazine.net/news/20250522-github-copilot-coding-agent-error/](https://gigazine.net/news/20250522-github-copilot-coding-agent-error/)  
2. AI Agents Are Here. So Are the Threats. \- Palo Alto Networks Unit 42, 5月 31, 2025にアクセス、 [https://unit42.paloaltonetworks.com/agentic-ai-threats/](https://unit42.paloaltonetworks.com/agentic-ai-threats/)  
3. あなたのIDE、時代遅れかも？話題のAIコーディング環境「Codex ..., 5月 31, 2025にアクセス、 [https://note.com/life\_to\_ai/n/n22dcfaeb7ff0](https://note.com/life_to_ai/n/n22dcfaeb7ff0)  
4. Tabnine tries to update files in wrong directory on IntelliJ IDEA ..., 5月 31, 2025にアクセス、 [https://github.com/codota/tabnine-intellij/issues/741](https://github.com/codota/tabnine-intellij/issues/741)  
5. Report on Rule Violations and Unacceptable Behavior \- Bug ..., 5月 31, 2025にアクセス、 [https://forum.cursor.com/t/report-on-rule-violations-and-unacceptable-behavior/74956](https://forum.cursor.com/t/report-on-rule-violations-and-unacceptable-behavior/74956)  
6. AI in Corporate: The Coming Wave of Disastrous Blunders \- DEV ..., 5月 31, 2025にアクセス、 [https://dev.to/mtsammy40/ai-in-corporate-the-coming-wave-of-disastrous-blunders-367h](https://dev.to/mtsammy40/ai-in-corporate-the-coming-wave-of-disastrous-blunders-367h)  
7. Cursor AIとMCP (Anthropic)セキュリティリスクの理解 \#生成AI \- Qiita, 5月 31, 2025にアクセス、 [https://qiita.com/RepKuririn/items/52f2f1ed5648e52ffa14](https://qiita.com/RepKuririn/items/52f2f1ed5648e52ffa14)  
8. 狙われる「サービスとしてのAI」 – AWS AIサービスへの攻撃の理解とTrend Vision One™による可視化 | トレンドマイクロ \- Trend Micro, 5月 31, 2025にアクセス、 [https://www.trendmicro.com/ja\_jp/research/25/c/detecting-attacks-on-aws-ai-services-with-trend-vision-one.html](https://www.trendmicro.com/ja_jp/research/25/c/detecting-attacks-on-aws-ai-services-with-trend-vision-one.html)  
9. Creating a custom model for GitHub Copilot \- GitHub Enterprise Cloud Docs, 5月 31, 2025にアクセス、 [https://docs.github.com/enterprise-cloud@latest/copilot/customizing-copilot/creating-a-custom-model-for-github-copilot](https://docs.github.com/enterprise-cloud@latest/copilot/customizing-copilot/creating-a-custom-model-for-github-copilot)  
10. Linting with VSG sometimes lints unexpected files · Issue \#748 · TerosTechnology/vscode-terosHDL \- GitHub, 5月 31, 2025にアクセス、 [https://github.com/TerosTechnology/vscode-terosHDL/issues/748](https://github.com/TerosTechnology/vscode-terosHDL/issues/748)  
11. 2025年に試すべき最高のAIコパイロットツール9選 \- TextCortex, 5月 31, 2025にアクセス、 [https://textcortex.com/ja/post/best-ai-copilot-tools](https://textcortex.com/ja/post/best-ai-copilot-tools)  
12. Safe, responsible and effective use of LLMs \- DNV Technology Insights, 5月 31, 2025にアクセス、 [https://technologyinsights.dnv.com/safe-responsible-and-effective-use-of-llms/](https://technologyinsights.dnv.com/safe-responsible-and-effective-use-of-llms/)  
13. A Deep Dive Into Large Language Model Code Generation Mistakes: What and Why?, 5月 31, 2025にアクセス、 [https://arxiv.org/html/2411.01414v1](https://arxiv.org/html/2411.01414v1)  
14. Large Language Models and Simple, Stupid Bugs | Request PDF, 5月 31, 2025にアクセス、 [https://www.researchgate.net/publication/372329218\_Large\_Language\_Models\_and\_Simple\_Stupid\_Bugs](https://www.researchgate.net/publication/372329218_Large_Language_Models_and_Simple_Stupid_Bugs)  
15. GitHub Copilotとは？使い方や料金、使えない場合の対処法などを解説 | AInformation, 5月 31, 2025にアクセス、 [https://ainformation.jp/article/3733](https://ainformation.jp/article/3733)  
16. OWASP Top 10 for LLMでLLM Applicationsのセキュリティについて ..., 5月 31, 2025にアクセス、 [https://zenn.dev/loglass/articles/41b1b4e809aac9](https://zenn.dev/loglass/articles/41b1b4e809aac9)  
17. Before you build agentic AI, understand the confused deputy problem, 5月 31, 2025にアクセス、 [https://www.hashicorp.com/blog/before-you-build-agentic-ai-understand-the-confused-deputy-problem](https://www.hashicorp.com/blog/before-you-build-agentic-ai-understand-the-confused-deputy-problem)  
18. エージェントAIは壊れてもいい：マルチAI構造という防御設計のすすめ \- Zenn, 5月 31, 2025にアクセス、 [https://zenn.dev/takamiey/articles/3a33ff793a772f](https://zenn.dev/takamiey/articles/3a33ff793a772f)  
19. GitHub Copilotの7大トラブル完全解決！初心者の疑問を解消 \- note, 5月 31, 2025にアクセス、 [https://note.com/lucky\_ram7202/n/n80020852bc82](https://note.com/lucky_ram7202/n/n80020852bc82)  
20. copilot勝手にインストールや起動を防ぐ方法と削除までの徹底対策ガイド | AI \- 株式会社アシスト, 5月 31, 2025にアクセス、 [https://assist-all.co.jp/column/ai/20250529-4718/](https://assist-all.co.jp/column/ai/20250529-4718/)  
21. Copilotは削除できない？勝手にインストールされた際の対処法も解説 | AI・ChatGPTを活用した社内DXと人材育成のプロ集団｜株式会社エヌイチ, 5月 31, 2025にアクセス、 [https://n1-inc.co.jp/copilot-sakujo/](https://n1-inc.co.jp/copilot-sakujo/)  
22. Qt AIアシスタント, 5月 31, 2025にアクセス、 [https://www.qt.io/ja-jp/product/ai-assistant](https://www.qt.io/ja-jp/product/ai-assistant)  
23. IntelliJ IDEAの開発環境設定と確認手順ガイド, 5月 31, 2025にアクセス、 [https://www.issoh.co.jp/tech/details/4377/](https://www.issoh.co.jp/tech/details/4377/)  
24. Adding repository custom instructions for GitHub Copilot \- GitHub Docs, 5月 31, 2025にアクセス、 [https://docs.github.com/en/copilot/customizing-copilot/adding-repository-custom-instructions-for-github-copilot](https://docs.github.com/en/copilot/customizing-copilot/adding-repository-custom-instructions-for-github-copilot)  
25. Excluding content from GitHub Copilot \- GitHub Docs, 5月 31, 2025にアクセス、 [https://docs.github.com/en/copilot/managing-copilot/configuring-and-auditing-content-exclusion/excluding-content-from-github-copilot](https://docs.github.com/en/copilot/managing-copilot/configuring-and-auditing-content-exclusion/excluding-content-from-github-copilot)  
26. Guiding AI Coding Tool Adoption with Intention: Best Practices for ..., 5月 31, 2025にアクセス、 [https://jellyfish.co/blog/ai-coding-tool-adoption-best-practices/](https://jellyfish.co/blog/ai-coding-tool-adoption-best-practices/)  
27. Best Practices for Using AI in Software Development 2025 \- Leanware, 5月 31, 2025にアクセス、 [https://www.leanware.co/insights/best-practices-ai-software-development](https://www.leanware.co/insights/best-practices-ai-software-development)  
28. Branching Out: 4 Git Workflows for Collaborating on ML | Towards ..., 5月 31, 2025にアクセス、 [https://towardsdatascience.com/branching-out-4-git-workflows-for-collaborating-on-ml/](https://towardsdatascience.com/branching-out-4-git-workflows-for-collaborating-on-ml/)  
29. What are Git version control best practices? \- GitLab, 5月 31, 2025にアクセス、 [https://about.gitlab.com/topics/version-control/version-control-best-practices/](https://about.gitlab.com/topics/version-control/version-control-best-practices/)  
30. Reverting changes to a single file \- why are so simple things so difficult? : r/git \- Reddit, 5月 31, 2025にアクセス、 [https://www.reddit.com/r/git/comments/11tqzmc/reverting\_changes\_to\_a\_single\_file\_why\_are\_so/](https://www.reddit.com/r/git/comments/11tqzmc/reverting_changes_to_a_single_file_why_are_so/)  
31. Git Clean: Remove Untracked Files and Keep Repos Tidy | DataCamp, 5月 31, 2025にアクセス、 [https://www.datacamp.com/tutorial/git-clean](https://www.datacamp.com/tutorial/git-clean)  
32. Best Practices for Git and Version Control \- DEV Community, 5月 31, 2025にアクセス、 [https://dev.to/aneeqakhan/best-practices-for-git-and-version-control-588m](https://dev.to/aneeqakhan/best-practices-for-git-and-version-control-588m)  
33. Adopting AI tools in your development workflow \- Graphite, 5月 31, 2025にアクセス、 [https://graphite.dev/guides/adopting-ai-tools-development-workflow](https://graphite.dev/guides/adopting-ai-tools-development-workflow)  
34. Script monitor \- AI Operations Management \- SaaS, 5月 31, 2025にアクセス、 [https://docs.microfocus.com/doc/116/saas/115-scriptmonitor](https://docs.microfocus.com/doc/116/saas/115-scriptmonitor)  
35. File and folder change monitoring | ManageEngine ADAudit Plus, 5月 31, 2025にアクセス、 [https://www.manageengine.com/products/active-directory-audit/monitor-file-changes-windows.html](https://www.manageengine.com/products/active-directory-audit/monitor-file-changes-windows.html)  
36. Potential for China Cyber Response to Heightened U.S.–China Tensions \- CISA, 5月 31, 2025にアクセス、 [https://www.cisa.gov/news-events/cybersecurity-advisories/aa20-275a](https://www.cisa.gov/news-events/cybersecurity-advisories/aa20-275a)  
37. Diff Checker \- GitClear, 5月 31, 2025にアクセス、 [https://www.gitclear.com/rich\_diff\_checker](https://www.gitclear.com/rich_diff_checker)  
38. Forewarned is Forearmed: A Survey on Large Language Model-based Agents in Autonomous Cyberattacks \- arXiv, 5月 31, 2025にアクセス、 [https://arxiv.org/html/2505.12786v1](https://arxiv.org/html/2505.12786v1)  
39. Blog \- Documentation, 5月 31, 2025にアクセス、 [https://docs.lit.ai/blog/](https://docs.lit.ai/blog/)  
40. Release tools \- execute \`git commit \--dry-run\` to prevent unexpected crashes · Issue \#17967 · ckeditor/ckeditor5 \- GitHub, 5月 31, 2025にアクセス、 [https://github.com/ckeditor/ckeditor5/issues/17967](https://github.com/ckeditor/ckeditor5/issues/17967)  
41. NeMo Guardrails | NVIDIA Developer, 5月 31, 2025にアクセス、 [https://developer.nvidia.com/nemo-guardrails](https://developer.nvidia.com/nemo-guardrails)  
42. Top 10 Common Magento 2 Vulnerabilities Every Developer Must Know \- MoldStud, 5月 31, 2025にアクセス、 [https://moldstud.com/articles/p-top-10-common-magento-2-vulnerabilities-every-developer-must-know](https://moldstud.com/articles/p-top-10-common-magento-2-vulnerabilities-every-developer-must-know)  
43. Use Tomcat with Java SecurityManager? \- Stack Overflow, 5月 31, 2025にアクセス、 [https://stackoverflow.com/questions/1277276/use-tomcat-with-java-securitymanager](https://stackoverflow.com/questions/1277276/use-tomcat-with-java-securitymanager)  
44. Dataset Diffs \- Oxen.ai, 5月 31, 2025にアクセス、 [https://docs.oxen.ai/concepts/diffs](https://docs.oxen.ai/concepts/diffs)  
45. guardrails-ai repositories \- GitHub, 5月 31, 2025にアクセス、 [https://github.com/orgs/guardrails-ai/repositories](https://github.com/orgs/guardrails-ai/repositories)  
46. 【2024年最新】生成AIの問題事例4選｜情報漏洩から著作権まで \- メタバース総研, 5月 31, 2025にアクセス、 [https://metaversesouken.com/ai/generative\_ai/trouble-cases/](https://metaversesouken.com/ai/generative_ai/trouble-cases/)  
47. AIコードジェネレーターが及ぼす可能性のある法的な問題とその解決策 \- Open Legal Community, 5月 31, 2025にアクセス、 [https://openlegalcommunity.com/ai-code-generator-legal-issues/](https://openlegalcommunity.com/ai-code-generator-legal-issues/)  
48. GitHub Copilotと“人間特権”, 5月 31, 2025にアクセス、 [https://zenn.dev/uhyo/articles/github-copilot-and-human-privilege](https://zenn.dev/uhyo/articles/github-copilot-and-human-privilege)  
49. Skylar W. Schossberger \- Hawley Troxell, 5月 31, 2025にアクセス、 [https://hawleytroxell.com/the-firm/people/skylar-schossberger/](https://hawleytroxell.com/the-firm/people/skylar-schossberger/)  
50. 360 questions with answers in LEGAL | Science topic \- ResearchGate, 5月 31, 2025にアクセス、 [https://www.researchgate.net/topic/Legal](https://www.researchgate.net/topic/Legal)