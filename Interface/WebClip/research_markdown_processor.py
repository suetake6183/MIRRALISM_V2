#!/usr/bin/env python3
"""
ãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
=====================================

MIRRALISM V2 WebClipæ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ 
ç›®çš„: Gemini/Perplexityãƒªã‚µãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆå‡¦ç†

ä½œæˆè€…: æŠ€è¡“è²¬ä»»è€…
ä½œæˆæ—¥: 2025å¹´6æœˆ6æ—¥
è¨­è¨ˆæ€æƒ³: WebClipã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µã«ã‚ˆã‚‹çµ±ä¸€ä½“é¨“
"""

import json
import logging
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# æ—¢å­˜WebClipã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .motivation_analyzer import WebClipMotivationAnalyzer
from .yaml_processor import YAMLFrontmatterProcessor


class ResearchMarkdownProcessor:
    """ãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, project_root: Optional[Path] = None):
        """
        ãƒªã‚µãƒ¼ãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        
        Args:
            project_root: MIRRALISMãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
        """
        self.project_root = project_root or Path(__file__).parent.parent.parent
        self.setup_logging()
        
        # æ—¢å­˜WebClipã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
        self.motivation_analyzer = WebClipMotivationAnalyzer(project_root)
        self.yaml_processor = YAMLFrontmatterProcessor(project_root)
        
        # ãƒªã‚µãƒ¼ãƒç‰¹åŒ–ã‚·ã‚¹ãƒ†ãƒ 
        self.research_sources = self._initialize_research_sources()
        self.processing_stats = {
            "total_processed": 0,
            "successful_processing": 0,
            "source_detection_accuracy": 0.0,
            "research_files_by_source": {}
        }
        
        self.logger.info("âœ… ãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - RESEARCH_MARKDOWN - %(levelname)s - %(message)s"
        )
        self.logger = logging.getLogger(__name__)

    def process_research_markdown(
        self,
        markdown_file_path: str,
        user_context: Optional[Dict] = None,
        save_to_file: bool = True
    ) -> Dict[str, Any]:
        """
        ãƒªã‚µãƒ¼ãƒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨å‡¦ç†
        
        CTOã®è¦æ±‚:
        - ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•è§£æ
        - ãƒªã‚µãƒ¼ãƒã‚½ãƒ¼ã‚¹æ¤œå‡ºï¼ˆGemini/Perplexity/Claudeï¼‰
        - æ—¢å­˜å‹•æ©Ÿåˆ†æã‚·ã‚¹ãƒ†ãƒ é©ç”¨ï¼š"ãªãœã“ã®ãƒªã‚µãƒ¼ãƒã‚’ã—ãŸã®ã‹ï¼Ÿ"
        - WebClipã¨åŒæ§˜ã®çµ±ä¸€ä½“é¨“
        
        Args:
            markdown_file_path: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            save_to_file: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ•ãƒ©ã‚°
            
        Returns:
            å‡¦ç†çµæœ
        """
        try:
            self.processing_stats["total_processed"] += 1
            
            file_path = Path(markdown_file_path)
            self.logger.info(f"ğŸ” ãƒªã‚µãƒ¼ãƒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡¦ç†é–‹å§‹: {file_path.name}")
            
            # 1. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ»è§£æ
            markdown_analysis = self._analyze_markdown_file(file_path)
            
            # 2. ãƒªã‚µãƒ¼ãƒã‚½ãƒ¼ã‚¹æ¤œå‡ºãƒ»åˆ†é¡
            source_detection = self._detect_research_source(markdown_analysis)
            
            # 3. ãƒªã‚µãƒ¼ãƒå†…å®¹ã®æ§‹é€ åŒ–åˆ†æ
            content_analysis = self._analyze_research_content(markdown_analysis, source_detection)
            
            # 4. æ—¢å­˜å‹•æ©Ÿåˆ†æã‚·ã‚¹ãƒ†ãƒ é©ç”¨ï¼ˆWebClipã¨åŒã˜ã‚¨ãƒ³ã‚¸ãƒ³ï¼‰
            motivation_result = self.motivation_analyzer.analyze_clip_motivation(
                article_content=content_analysis["structured_content"],
                article_url=f"research_file://{file_path.name}",
                article_title=content_analysis["research_title"],
                user_context={
                    **(user_context or {}),
                    "input_type": "research_markdown",
                    "research_source": source_detection["detected_source"],
                    "research_depth": content_analysis["research_depth"]
                }
            )
            
            # 5. ãƒªã‚µãƒ¼ãƒç‰¹åŒ–ã®è³ªå•ãƒ»æ´å¯Ÿç”Ÿæˆ
            research_insights = self._generate_research_insights(
                content_analysis, source_detection, motivation_result
            )
            
            # 6. YAML frontmatterç”Ÿæˆï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆ©ç”¨ï¼‰
            yaml_result = self.yaml_processor.process_webclip_frontmatter(
                article_title=f"[{source_detection['detected_source']}] {content_analysis['research_title']}",
                article_url=f"research_file://{file_path.name}",
                article_content=content_analysis["structured_content"],
                clip_metadata={
                    "input_type": "research_markdown",
                    "source_file": str(file_path),
                    "research_source": source_detection["detected_source"],
                    "research_depth": content_analysis["research_depth"],
                    "processed_at": datetime.now(timezone.utc).isoformat()
                },
                user_context=user_context
            )
            
            # 7. çµ±åˆçµæœæ§‹ç¯‰
            integrated_result = self._build_research_result(
                markdown_analysis, source_detection, content_analysis,
                motivation_result, research_insights, yaml_result
            )
            
            # 8. ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_result = None
            if save_to_file:
                save_result = self._save_research_file(integrated_result, file_path)
            
            # çµ±è¨ˆæ›´æ–°
            self.processing_stats["successful_processing"] += 1
            self._update_source_stats(source_detection["detected_source"])
            
            result = {
                "success": True,
                "file_path": str(file_path),
                "research_analysis": integrated_result,
                "save_result": save_result,
                "processing_stats": self.processing_stats.copy()
            }
            
            self.logger.info(
                f"âœ… ãƒªã‚µãƒ¼ãƒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡¦ç†å®Œäº†: {file_path.name} "
                f"[{source_detection['detected_source']}]"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒªã‚µãƒ¼ãƒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_path": markdown_file_path,
                "processing_stats": self.processing_stats.copy()
            }

    def _analyze_markdown_file(self, file_path: Path) -> Dict[str, Any]:
        """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«è§£æ"""
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            file_stats = file_path.stat()
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ§‹é€ è§£æ
            structure = self._parse_markdown_structure(content)
            
            return {
                "file_path": str(file_path),
                "file_name": file_path.name,
                "file_size": file_stats.st_size,
                "created_time": datetime.fromtimestamp(file_stats.st_ctime, timezone.utc).isoformat(),
                "modified_time": datetime.fromtimestamp(file_stats.st_mtime, timezone.utc).isoformat(),
                "raw_content": content,
                "content_length": len(content),
                "structure": structure,
                "encoding": "utf-8"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _parse_markdown_structure(self, content: str) -> Dict[str, Any]:
        """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ§‹é€ è§£æ"""
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æŠ½å‡º
        headers = re.findall(r'^(#{1,6})\s+(.+)$', content, re.MULTILINE)
        
        # ãƒªãƒ³ã‚¯æŠ½å‡º
        links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
        
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡º
        code_blocks = re.findall(r'```(\w*)\n(.*?)\n```', content, re.DOTALL)
        
        # ç®‡æ¡æ›¸ãæŠ½å‡º
        bullet_points = re.findall(r'^[\*\-\+]\s+(.+)$', content, re.MULTILINE)
        
        # ç•ªå·ä»˜ããƒªã‚¹ãƒˆæŠ½å‡º
        numbered_lists = re.findall(r'^\d+\.\s+(.+)$', content, re.MULTILINE)
        
        return {
            "headers": [{"level": len(h[0]), "text": h[1]} for h in headers],
            "links": [{"text": l[0], "url": l[1]} for l in links],
            "code_blocks": [{"language": cb[0], "code": cb[1]} for cb in code_blocks],
            "bullet_points": bullet_points,
            "numbered_lists": numbered_lists,
            "total_headers": len(headers),
            "total_links": len(links),
            "total_code_blocks": len(code_blocks),
            "paragraph_count": len([p for p in content.split('\n\n') if p.strip()])
        }

    def _detect_research_source(self, markdown_analysis: Dict) -> Dict[str, Any]:
        """ãƒªã‚µãƒ¼ãƒã‚½ãƒ¼ã‚¹æ¤œå‡ºãƒ»åˆ†é¡"""
        
        content = markdown_analysis["raw_content"].lower()
        file_name = markdown_analysis["file_name"].lower()
        
        # ã‚½ãƒ¼ã‚¹æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        source_patterns = {
            "gemini": [
                "google gemini", "gemini pro", "gemini advanced", "bard",
                "ai.google.com", "gemini.google", "generated by gemini"
            ],
            "perplexity": [
                "perplexity", "perplexity.ai", "pplx", "perplexity pro",
                "generated by perplexity", "perplexity search"
            ],
            "claude": [
                "claude", "anthropic", "claude.ai", "claude pro",
                "generated by claude", "claude sonnet", "claude opus"
            ],
            "chatgpt": [
                "chatgpt", "openai", "gpt-4", "gpt-3.5", "chat.openai.com",
                "generated by chatgpt"
            ],
            "copilot": [
                "copilot", "microsoft copilot", "bing chat", "edge copilot"
            ]
        }
        
        # æ¤œå‡ºå®Ÿè¡Œ
        detected_sources = []
        confidence_scores = {}
        
        for source, patterns in source_patterns.items():
            matches = 0
            for pattern in patterns:
                if pattern in content or pattern in file_name:
                    matches += 1
            
            if matches > 0:
                confidence = min(matches / len(patterns), 1.0)
                detected_sources.append(source)
                confidence_scores[source] = confidence
        
        # ä¸»è¦ã‚½ãƒ¼ã‚¹æ±ºå®š
        if detected_sources:
            primary_source = max(confidence_scores.keys(), key=lambda x: confidence_scores[x])
            confidence = confidence_scores[primary_source]
        else:
            primary_source = "unknown"
            confidence = 0.0
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã®æ¨æ¸¬
        filename_hints = self._extract_filename_hints(markdown_analysis["file_name"])
        
        return {
            "detected_source": primary_source,
            "confidence": confidence,
            "all_detected_sources": detected_sources,
            "confidence_scores": confidence_scores,
            "filename_hints": filename_hints,
            "detection_method": "pattern_matching" if detected_sources else "fallback"
        }

    def _extract_filename_hints(self, filename: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã®ãƒ’ãƒ³ãƒˆæŠ½å‡º"""
        
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        date_patterns = [
            r'(\d{4}[-_]\d{2}[-_]\d{2})',  # YYYY-MM-DD
            r'(\d{2}[-_]\d{2}[-_]\d{4})',  # MM-DD-YYYY
            r'(\d{8})',                    # YYYYMMDD
        ]
        
        dates = []
        for pattern in date_patterns:
            matches = re.findall(pattern, filename)
            dates.extend(matches)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = re.findall(r'[a-zA-Z]{3,}', filename.replace('.md', ''))
        
        return {
            "extracted_dates": dates,
            "keywords": keywords,
            "extension": Path(filename).suffix,
            "basename": Path(filename).stem
        }

    def _analyze_research_content(self, markdown_analysis: Dict, source_detection: Dict) -> Dict[str, Any]:
        """ãƒªã‚µãƒ¼ãƒå†…å®¹ã®æ§‹é€ åŒ–åˆ†æ"""
        
        content = markdown_analysis["raw_content"]
        structure = markdown_analysis["structure"]
        
        # ã‚¿ã‚¤ãƒˆãƒ«æ¨å®š
        research_title = self._extract_research_title(structure, markdown_analysis["file_name"])
        
        # ç ”ç©¶æ·±åº¦è©•ä¾¡
        research_depth = self._assess_research_depth(structure, content)
        
        # ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
        main_topics = self._extract_main_topics(structure, content)
        
        # æƒ…å ±æºãƒ»å¼•ç”¨åˆ†æ
        sources_analysis = self._analyze_information_sources(structure)
        
        # æ§‹é€ åŒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆï¼ˆå‹•æ©Ÿåˆ†æç”¨ï¼‰
        structured_content = self._create_structured_content_for_analysis(
            content, research_title, main_topics
        )
        
        return {
            "research_title": research_title,
            "research_depth": research_depth,
            "main_topics": main_topics,
            "sources_analysis": sources_analysis,
            "structured_content": structured_content,
            "content_type": "research_markdown",
            "estimated_read_time": self._estimate_read_time(len(content)),
            "complexity_level": self._assess_complexity_level(structure, content)
        }

    def _extract_research_title(self, structure: Dict, filename: str) -> str:
        """ãƒªã‚µãƒ¼ãƒã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        
        # æœ€åˆã®H1ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½¿ç”¨
        headers = structure.get("headers", [])
        for header in headers:
            if header["level"] == 1:
                return header["text"]
        
        # H1ãŒãªã„å ´åˆã¯H2ã‚’ä½¿ç”¨
        for header in headers:
            if header["level"] == 2:
                return header["text"]
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç”Ÿæˆ
        basename = Path(filename).stem
        return f"Research: {basename.replace('_', ' ').replace('-', ' ')}"

    def _assess_research_depth(self, structure: Dict, content: str) -> str:
        """ç ”ç©¶æ·±åº¦è©•ä¾¡"""
        
        # æ·±åº¦æŒ‡æ¨™
        header_count = structure.get("total_headers", 0)
        link_count = structure.get("total_links", 0)
        code_count = structure.get("total_code_blocks", 0)
        paragraph_count = structure.get("paragraph_count", 0)
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        depth_score = (
            header_count * 0.3 +
            link_count * 0.2 +
            code_count * 0.1 +
            paragraph_count * 0.4
        )
        
        if depth_score > 20:
            return "comprehensive"  # åŒ…æ‹¬çš„
        elif depth_score > 10:
            return "detailed"       # è©³ç´°
        elif depth_score > 5:
            return "moderate"       # ä¸­ç¨‹åº¦
        else:
            return "basic"          # åŸºæœ¬çš„

    def _extract_main_topics(self, structure: Dict, content: str) -> List[str]:
        """ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º"""
        
        topics = []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
        headers = structure.get("headers", [])
        for header in headers:
            if header["level"] <= 3:  # H1-H3ã®ã¿
                topics.append(header["text"])
        
        # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        words = re.findall(r'\b[A-Za-z]{4,}\b', content.lower())
        word_freq = {}
        for word in words:
            if word not in ['this', 'that', 'with', 'from', 'they', 'have', 'were', 'been', 'their']:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # é »å‡ºä¸Šä½5å˜èªã‚’è¿½åŠ 
        frequent_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        topics.extend([word for word, freq in frequent_words if freq > 2])
        
        return topics[:10]  # æœ€å¤§10ãƒˆãƒ”ãƒƒã‚¯

    def _analyze_information_sources(self, structure: Dict) -> Dict[str, Any]:
        """æƒ…å ±æºãƒ»å¼•ç”¨åˆ†æ"""
        
        links = structure.get("links", [])
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æ
        domains = {}
        for link in links:
            url = link["url"]
            if url.startswith(('http://', 'https://')):
                try:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc
                    domains[domain] = domains.get(domain, 0) + 1
                except:
                    pass
        
        # ä¿¡é ¼æ€§è©•ä¾¡
        trusted_domains = {
            'wikipedia.org', 'nature.com', 'sciencedirect.com', 'arxiv.org',
            'pubmed.ncbi.nlm.nih.gov', 'scholar.google.com', 'ieee.org'
        }
        
        credibility_score = 0.0
        if domains:
            trusted_count = sum(1 for domain in domains.keys() if domain in trusted_domains)
            credibility_score = trusted_count / len(domains)
        
        return {
            "total_links": len(links),
            "unique_domains": len(domains),
            "domain_distribution": domains,
            "credibility_score": credibility_score,
            "has_citations": len(links) > 3
        }

    def _create_structured_content_for_analysis(
        self, content: str, title: str, topics: List[str]
    ) -> str:
        """å‹•æ©Ÿåˆ†æç”¨ã®æ§‹é€ åŒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ"""
        
        # æ—¢å­˜ã®å‹•æ©Ÿåˆ†æã‚·ã‚¹ãƒ†ãƒ ãŒå‡¦ç†ã—ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
        structured = f"""Research Title: {title}

Main Topics: {', '.join(topics[:5])}

Content Summary:
{content[:1000]}...

Research Context: This is a deep research document containing detailed analysis and insights.
"""
        
        return structured

    def _estimate_read_time(self, content_length: int) -> str:
        """èª­æ›¸æ™‚é–“æ¨å®š"""
        
        # å¹³å‡èª­æ›¸é€Ÿåº¦: 250å˜èª/åˆ†ï¼ˆè‹±èªï¼‰ã€400æ–‡å­—/åˆ†ï¼ˆæ—¥æœ¬èªï¼‰
        words_estimate = content_length / 5  # å¹³å‡å˜èªé•·5æ–‡å­—ã¨ä»®å®š
        minutes = max(1, int(words_estimate / 250))
        
        if minutes < 5:
            return f"{minutes}åˆ†"
        elif minutes < 15:
            return f"{minutes}åˆ†ï¼ˆä¸­ç¨‹åº¦ï¼‰"
        else:
            return f"{minutes}åˆ†ï¼ˆé•·æ–‡ç ”ç©¶ï¼‰"

    def _assess_complexity_level(self, structure: Dict, content: str) -> str:
        """è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«è©•ä¾¡"""
        
        # è¤‡é›‘åº¦æŒ‡æ¨™
        technical_terms = len(re.findall(r'\b[A-Z]{2,}\b', content))  # å°‚é–€ç”¨èª
        code_blocks = structure.get("total_code_blocks", 0)
        nested_headers = len([h for h in structure.get("headers", []) if h["level"] > 2])
        
        complexity_score = technical_terms * 0.4 + code_blocks * 0.3 + nested_headers * 0.3
        
        if complexity_score > 15:
            return "expert"
        elif complexity_score > 8:
            return "advanced"
        elif complexity_score > 3:
            return "intermediate"
        else:
            return "basic"

    def _generate_research_insights(
        self,
        content_analysis: Dict,
        source_detection: Dict,
        motivation_result: Dict
    ) -> Dict[str, Any]:
        """ãƒªã‚µãƒ¼ãƒç‰¹åŒ–æ´å¯Ÿç”Ÿæˆ"""
        
        # ãƒªã‚µãƒ¼ãƒç‰¹åŒ–ã®è³ªå•ç”Ÿæˆ
        research_questions = self._generate_research_questions(
            content_analysis, source_detection
        )
        
        # æ´»ç”¨ææ¡ˆï¼ˆãƒªã‚µãƒ¼ãƒç‰¹åŒ–ï¼‰
        utilization_suggestions = self._generate_research_utilization_suggestions(
            content_analysis, motivation_result
        )
        
        # ãƒªã‚µãƒ¼ãƒå“è³ªè©•ä¾¡
        quality_assessment = self._assess_research_quality(
            content_analysis, source_detection
        )
        
        return {
            "research_questions": research_questions,
            "utilization_suggestions": utilization_suggestions,
            "quality_assessment": quality_assessment,
            "research_type": self._classify_research_type(content_analysis),
            "follow_up_recommendations": self._generate_follow_up_recommendations(content_analysis)
        }

    def _generate_research_questions(
        self, content_analysis: Dict, source_detection: Dict
    ) -> List[str]:
        """ãƒªã‚µãƒ¼ãƒç‰¹åŒ–è³ªå•ç”Ÿæˆ"""
        
        questions = []
        
        research_depth = content_analysis["research_depth"]
        detected_source = source_detection["detected_source"]
        main_topics = content_analysis.get("main_topics", [])
        
        # æ·±åº¦ã«å¿œã˜ãŸè³ªå•
        if research_depth == "comprehensive":
            questions.append("ã“ã®åŒ…æ‹¬çš„ãªãƒªã‚µãƒ¼ãƒã¯ã€ã©ã®ã‚ˆã†ãªå¤§ããªæ±ºå®šã‚„æˆ¦ç•¥ã®ãŸã‚ã§ã—ã‚‡ã†ã‹ï¼Ÿ")
        elif research_depth == "detailed":
            questions.append("è©³ç´°ãªèª¿æŸ»ã‚’ã•ã‚Œã¾ã—ãŸãŒã€å…·ä½“çš„ã«ã©ã®éƒ¨åˆ†ãŒæœ€ã‚‚é‡è¦ã§ã—ãŸã‹ï¼Ÿ")
        else:
            questions.append("ã“ã®ãƒªã‚µãƒ¼ãƒã®ãã£ã‹ã‘ã¨ãªã£ãŸèª²é¡Œã‚„ç–‘å•ã¯ä½•ã§ã—ãŸã‹ï¼Ÿ")
        
        # ã‚½ãƒ¼ã‚¹ã«å¿œã˜ãŸè³ªå•
        if detected_source == "gemini":
            questions.append("Geminiã‚’é¸æŠã•ã‚ŒãŸç†ç”±ã¯ä½•ã§ã—ã‚‡ã†ã‹ï¼Ÿä»–ã®AIã¨ã®æ¯”è¼ƒæ¤œè¨ã¯ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ")
        elif detected_source == "perplexity":
            questions.append("Perplexityã®æ¤œç´¢çµæœã§ã€æœ€ã‚‚ä¾¡å€¤ãŒã‚ã£ãŸæƒ…å ±ã¯ä½•ã§ã—ãŸã‹ï¼Ÿ")
        elif detected_source == "claude":
            questions.append("Claudeã¨ã®å¯¾è©±ã§ã€äºˆæœŸã—ãªã„æ´å¯Ÿã‚„ç™ºè¦‹ã¯ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ")
        
        # ãƒˆãƒ”ãƒƒã‚¯ã«å¿œã˜ãŸè³ªå•
        if main_topics:
            primary_topic = main_topics[0]
            questions.append(f"ã€Œ{primary_topic}ã€ã«ã¤ã„ã¦ã€ä»Šå¾Œã•ã‚‰ã«æ·±ãèª¿ã¹ãŸã„å´é¢ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ")
        
        return questions[:3]  # æœ€å¤§3ã¤ã®è³ªå•

    def _generate_research_utilization_suggestions(
        self, content_analysis: Dict, motivation_result: Dict
    ) -> List[Dict[str, str]]:
        """ãƒªã‚µãƒ¼ãƒæ´»ç”¨ææ¡ˆç”Ÿæˆ"""
        
        suggestions = []
        
        research_depth = content_analysis["research_depth"]
        complexity = content_analysis["complexity_level"]
        
        # æ·±åº¦åˆ¥ææ¡ˆ
        if research_depth == "comprehensive":
            suggestions.append({
                "type": "strategic",
                "suggestion": "æˆ¦ç•¥æ–‡æ›¸ã¨ã—ã¦æ•´ç†ã—ã€æ„æ€æ±ºå®šã®å‚è€ƒè³‡æ–™ã«ã™ã‚‹"
            })
            suggestions.append({
                "type": "sharing",
                "suggestion": "ãƒãƒ¼ãƒ ã‚„é–¢ä¿‚è€…ã¨å…±æœ‰ã—ã€é›†åˆçŸ¥ã¨ã—ã¦æ´»ç”¨ã™ã‚‹"
            })
        else:
            suggestions.append({
                "type": "immediate",
                "suggestion": "å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã«è½ã¨ã—è¾¼ã‚€"
            })
        
        # è¤‡é›‘åº¦åˆ¥ææ¡ˆ
        if complexity in ["expert", "advanced"]:
            suggestions.append({
                "type": "learning",
                "suggestion": "å°‚é–€çŸ¥è­˜ã¨ã—ã¦ä½“ç³»çš„ã«æ•´ç†ã—ã€å­¦ç¿’è³‡æ–™åŒ–ã™ã‚‹"
            })
        else:
            suggestions.append({
                "type": "practical",
                "suggestion": "å®Ÿè·µçš„ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ´»ç”¨ã™ã‚‹"
            })
        
        # å‹•æ©Ÿçµæœã‹ã‚‰ã®ææ¡ˆ
        if motivation_result.get("success"):
            analysis = motivation_result.get("analysis", {})
            primary_motivation = analysis.get("motivation_estimation", {}).get("primary_motivation")
            
            if primary_motivation and primary_motivation.get("type") == "mirralism_application":
                suggestions.append({
                    "type": "mirralism",
                    "suggestion": "MIRRALISMã®è¨­è¨ˆãƒ»é–‹ç™ºã«ç›´æ¥é©ç”¨ã™ã‚‹"
                })
        
        return suggestions[:4]  # æœ€å¤§4ã¤ã®ææ¡ˆ

    def _assess_research_quality(
        self, content_analysis: Dict, source_detection: Dict
    ) -> Dict[str, Any]:
        """ãƒªã‚µãƒ¼ãƒå“è³ªè©•ä¾¡"""
        
        # å“è³ªæŒ‡æ¨™
        depth_score = {"basic": 1, "moderate": 2, "detailed": 3, "comprehensive": 4}[
            content_analysis["research_depth"]
        ]
        
        complexity_score = {"basic": 1, "intermediate": 2, "advanced": 3, "expert": 4}[
            content_analysis["complexity_level"]
        ]
        
        source_confidence = source_detection["confidence"]
        
        sources_analysis = content_analysis["sources_analysis"]
        credibility_score = sources_analysis["credibility_score"]
        
        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        overall_quality = (
            depth_score * 0.3 +
            complexity_score * 0.2 +
            source_confidence * 0.2 +
            credibility_score * 0.3
        ) / 4.0
        
        # å“è³ªãƒ©ãƒ™ãƒ«
        if overall_quality > 0.8:
            quality_label = "excellent"
        elif overall_quality > 0.6:
            quality_label = "good"
        elif overall_quality > 0.4:
            quality_label = "acceptable"
        else:
            quality_label = "needs_improvement"
        
        return {
            "overall_quality": overall_quality,
            "quality_label": quality_label,
            "depth_score": depth_score,
            "complexity_score": complexity_score,
            "source_confidence": source_confidence,
            "credibility_score": credibility_score,
            "recommendations": self._generate_quality_recommendations(overall_quality)
        }

    def _generate_quality_recommendations(self, quality_score: float) -> List[str]:
        """å“è³ªæ”¹å–„æ¨å¥¨äº‹é …"""
        
        recommendations = []
        
        if quality_score < 0.6:
            recommendations.append("ã‚ˆã‚Šå¤šãã®ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‹ã‚‰ã®æƒ…å ±åé›†ã‚’æ¨å¥¨")
            recommendations.append("å°‚é–€å®¶ã®æ„è¦‹ã‚„å­¦è¡“è«–æ–‡ã®å‚ç…§ã‚’æ¤œè¨")
        
        if quality_score < 0.8:
            recommendations.append("é–¢é€£ã™ã‚‹æœ€æ–°ã®ç ”ç©¶ã‚„å‹•å‘ã‚‚èª¿æŸ»å¯¾è±¡ã«å«ã‚ã‚‹")
            recommendations.append("è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰ã®åˆ†æã‚’è¿½åŠ ã™ã‚‹")
        
        recommendations.append("å®šæœŸçš„ãªæƒ…å ±æ›´æ–°ã¨è¦‹ç›´ã—ã‚’è¨ˆç”»ã™ã‚‹")
        
        return recommendations

    def _classify_research_type(self, content_analysis: Dict) -> str:
        """ãƒªã‚µãƒ¼ãƒã‚¿ã‚¤ãƒ—åˆ†é¡"""
        
        main_topics = content_analysis.get("main_topics", [])
        complexity = content_analysis["complexity_level"]
        depth = content_analysis["research_depth"]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
        topic_text = " ".join(main_topics).lower()
        
        if any(word in topic_text for word in ["technology", "ai", "machine", "software", "system"]):
            return "technology_research"
        elif any(word in topic_text for word in ["business", "strategy", "market", "management"]):
            return "business_research"
        elif any(word in topic_text for word in ["science", "research", "study", "analysis"]):
            return "academic_research"
        elif any(word in topic_text for word in ["health", "medical", "healthcare"]):
            return "health_research"
        else:
            return "general_research"

    def _generate_follow_up_recommendations(self, content_analysis: Dict) -> List[str]:
        """ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—æ¨å¥¨äº‹é …"""
        
        recommendations = []
        
        research_type = self._classify_research_type(content_analysis)
        main_topics = content_analysis.get("main_topics", [])
        
        if research_type == "technology_research":
            recommendations.append("æœ€æ–°ã®æŠ€è¡“å‹•å‘ã®ç¶™ç¶šç›£è¦–")
            recommendations.append("å®Ÿè£…å¯èƒ½æ€§ã®æŠ€è¡“æ¤œè¨¼")
        elif research_type == "business_research":
            recommendations.append("ç«¶åˆåˆ†æã®å®Ÿæ–½")
            recommendations.append("å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®å®šé‡åˆ†æ")
        
        if main_topics:
            recommendations.append(f"ã€Œ{main_topics[0]}ã€ã®é–¢é€£åˆ†é‡ã§ã®è¿½åŠ èª¿æŸ»")
        
        recommendations.append("å°‚é–€å®¶ã‚„ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨ã®æ„è¦‹äº¤æ›")
        
        return recommendations[:3]

    def _build_research_result(
        self,
        markdown_analysis: Dict,
        source_detection: Dict,
        content_analysis: Dict,
        motivation_result: Dict,
        research_insights: Dict,
        yaml_result: Dict
    ) -> Dict[str, Any]:
        """çµ±åˆãƒªã‚µãƒ¼ãƒçµæœæ§‹ç¯‰"""
        
        return {
            "processing_timestamp": datetime.now(timezone.utc).isoformat(),
            "input_type": "research_markdown",
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            "file_analysis": {
                "file_path": markdown_analysis["file_path"],
                "file_name": markdown_analysis["file_name"],
                "file_size": markdown_analysis["file_size"],
                "created_time": markdown_analysis["created_time"],
                "modified_time": markdown_analysis["modified_time"]
            },
            
            # ã‚½ãƒ¼ã‚¹æ¤œå‡ºçµæœ
            "source_detection": source_detection,
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ
            "content_analysis": content_analysis,
            
            # å‹•æ©Ÿåˆ†æï¼ˆWebClipã¨åŒã˜ã‚¨ãƒ³ã‚¸ãƒ³ï¼‰
            "motivation_analysis": motivation_result,
            
            # ãƒªã‚µãƒ¼ãƒç‰¹åŒ–æ´å¯Ÿ
            "research_insights": research_insights,
            
            # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
            "structured_data": {
                "frontmatter": yaml_result.get("frontmatter", {}),
                "markdown_output": yaml_result.get("markdown_file", ""),
                "yaml_processing": yaml_result
            },
            
            # çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "metadata": {
                "system_version": "v2.0_webclip_extended",
                "processing_engine": "research_markdown_processor",
                "components_used": [
                    "motivation_analyzer", "yaml_processor", 
                    "research_content_analyzer", "source_detector"
                ]
            }
        }

    def _save_research_file(self, integrated_result: Dict, original_path: Path) -> Dict[str, Any]:
        """ãƒªã‚µãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜"""
        
        try:
            # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            save_dir = self.project_root / "Data" / "research_files"
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            source = integrated_result["source_detection"]["detected_source"]
            title = integrated_result["content_analysis"]["research_title"]
            safe_title = "".join(c for c in title if c.isalnum() or c in " -_")[:50]
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            markdown_filename = f"{timestamp}_{source}_{safe_title}.md"
            markdown_path = save_dir / markdown_filename
            
            markdown_content = integrated_result["structured_data"]["markdown_output"]
            with open(markdown_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # JSONåˆ†æçµæœä¿å­˜
            json_filename = f"{timestamp}_{source}_analysis.json"
            json_path = save_dir / json_filename
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(integrated_result, f, ensure_ascii=False, indent=2)
            
            return {
                "success": True,
                "markdown_file": str(markdown_path),
                "analysis_file": str(json_path),
                "original_file": str(original_path),
                "save_directory": str(save_dir)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒªã‚µãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def _update_source_stats(self, detected_source: str):
        """ã‚½ãƒ¼ã‚¹çµ±è¨ˆæ›´æ–°"""
        
        source_stats = self.processing_stats["research_files_by_source"]
        source_stats[detected_source] = source_stats.get(detected_source, 0) + 1

    def _initialize_research_sources(self) -> Dict[str, Dict]:
        """ãƒªã‚µãƒ¼ãƒã‚½ãƒ¼ã‚¹åˆæœŸåŒ–"""
        
        return {
            "gemini": {
                "name": "Google Gemini",
                "capabilities": ["reasoning", "research", "analysis"],
                "typical_outputs": ["detailed_analysis", "multi_perspective"]
            },
            "perplexity": {
                "name": "Perplexity AI",
                "capabilities": ["search", "synthesis", "citation"],
                "typical_outputs": ["sourced_research", "current_information"]
            },
            "claude": {
                "name": "Anthropic Claude",
                "capabilities": ["analysis", "reasoning", "writing"],
                "typical_outputs": ["structured_analysis", "thoughtful_insights"]
            },
            "chatgpt": {
                "name": "OpenAI ChatGPT",
                "capabilities": ["conversation", "analysis", "generation"],
                "typical_outputs": ["conversational_research", "creative_analysis"]
            },
            "copilot": {
                "name": "Microsoft Copilot",
                "capabilities": ["search", "productivity", "integration"],
                "typical_outputs": ["integrated_research", "productivity_focused"]
            }
        }

    def get_processing_statistics(self) -> Dict[str, Any]:
        """å‡¦ç†çµ±è¨ˆå–å¾—"""
        
        stats = self.processing_stats.copy()
        
        if stats["total_processed"] > 0:
            stats["success_rate"] = stats["successful_processing"] / stats["total_processed"]
            
            # ã‚½ãƒ¼ã‚¹æ¤œå‡ºç²¾åº¦è¨ˆç®—
            total_detections = sum(stats["research_files_by_source"].values())
            if total_detections > 0:
                stats["source_detection_accuracy"] = total_detections / stats["total_processed"]
        
        return stats


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    processor = ResearchMarkdownProcessor()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãƒ»ãƒ†ã‚¹ãƒˆ
    import tempfile
    
    sample_content = """# AI Personality Learning Research

## Overview
This is a comprehensive research on AI-driven personality learning systems using Google Gemini Advanced.

## Key Findings
- Machine learning algorithms can effectively analyze personality patterns
- Real-time processing achieves sub-2-second response times
- User engagement increases by 300% with personalized insights

## Implementation Strategies
1. **Data Collection**: Voice input integration
2. **Analysis Engine**: Multi-dimensional personality assessment
3. **User Interface**: Intuitive dialogue system

## Sources
- [Nature Journal](https://nature.com/ai-personality)
- [MIT Technology Review](https://technologyreview.com/personality-ai)
- [Google AI Blog](https://ai.googleblog.com/personality-learning)

Generated by Google Gemini Advanced on 2025-06-06
"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(sample_content)
        temp_path = f.name
    
    try:
        print("ğŸ§ª ãƒªã‚µãƒ¼ãƒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        result = processor.process_research_markdown(
            temp_path,
            user_context={"user_type": "CTO", "current_focus": "MIRRALISM development"},
            save_to_file=False
        )
        
        if result["success"]:
            analysis = result["research_analysis"]
            
            print(f"âœ… å‡¦ç†æˆåŠŸ")
            print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {analysis['file_analysis']['file_name']}")
            print(f"ğŸ” æ¤œå‡ºã‚½ãƒ¼ã‚¹: {analysis['source_detection']['detected_source']} "
                  f"(ä¿¡é ¼åº¦: {analysis['source_detection']['confidence']:.0%})")
            print(f"ğŸ“Š ãƒªã‚µãƒ¼ãƒæ·±åº¦: {analysis['content_analysis']['research_depth']}")
            print(f"ğŸ¯ è¤‡é›‘åº¦: {analysis['content_analysis']['complexity_level']}")
            print(f"â­ å“è³ª: {analysis['research_insights']['quality_assessment']['quality_label']}")
            
            # å‹•æ©Ÿåˆ†æçµæœ
            if analysis["motivation_analysis"].get("success"):
                motivation = analysis["motivation_analysis"]["analysis"]
                print(f"\nğŸ’­ å‹•æ©Ÿæ´å¯Ÿ: {motivation['dialogue']['interest_insight']}")
                print("â“ ãƒªã‚µãƒ¼ãƒè³ªå•:")
                for q in analysis["research_insights"]["research_questions"]:
                    print(f"   - {q}")
        else:
            print(f"âŒ å‡¦ç†å¤±æ•—: {result['error']}")
    
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        Path(temp_path).unlink()
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = processor.get_processing_statistics()
    print(f"\nğŸ“Š å‡¦ç†çµ±è¨ˆ")
    print(f"æˆåŠŸç‡: {stats.get('success_rate', 0):.0%}")
    print(f"ã‚½ãƒ¼ã‚¹æ¤œå‡ºç²¾åº¦: {stats.get('source_detection_accuracy', 0):.0%}")