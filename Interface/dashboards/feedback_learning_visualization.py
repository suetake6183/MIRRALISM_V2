#!/usr/bin/env python3
"""
MIRRALISM Feedback Learning Visualization
========================================

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ã®å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ 
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ç†è§£ã¨ä¿¡é ¼åº¦å‘ä¸Š
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class FeedbackLearningVisualizer:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_root: Optional[Path] = None):
        """å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        
        self.project_root = project_root or Path(__file__).parent.parent.parent
        self.feedback_log_path = self.project_root / ".mirralism" / "user_feedback_log.json"
        
        # æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.load_feedback_data()
        
        logger.info("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def load_feedback_data(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            if self.feedback_log_path.exists():
                with open(self.feedback_log_path, 'r', encoding='utf-8') as f:
                    self.feedback_data = json.load(f)
            else:
                self.feedback_data = {"reviews": [], "learned_rules": {}}
        except Exception as e:
            logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.feedback_data = {"reviews": [], "learned_rules": {}}
    
    def create_learning_progress_dashboard(self) -> Dict[str, Any]:
        """å­¦ç¿’é€²æ—ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ"""
        
        stats = self.feedback_data.get("learned_rules", {}).get("stats", {})
        
        dashboard = {
            "learning_overview": {
                "total_reviews": stats.get("total_reviewed", 0),
                "approval_rate": stats.get("approval_rate", 0),
                "learning_rules_count": len(self.feedback_data.get("learned_rules", {}).get("approval_patterns", [])) + 
                                      len(self.feedback_data.get("learned_rules", {}).get("rejection_patterns", [])),
                "learning_effectiveness": self._calculate_learning_effectiveness()
            },
            "pattern_learning": {
                "approval_patterns": self.feedback_data.get("learned_rules", {}).get("approval_patterns", []),
                "rejection_patterns": self.feedback_data.get("learned_rules", {}).get("rejection_patterns", [])
            },
            "recent_activity": self._get_recent_learning_activity(),
            "prediction_accuracy": self._calculate_prediction_accuracy(),
            "user_guidance": self._generate_user_guidance()
        }
        
        return dashboard
    
    def _calculate_learning_effectiveness(self) -> float:
        """å­¦ç¿’åŠ¹æœè¨ˆç®—"""
        rules = self.feedback_data.get("learned_rules", {})
        total_patterns = len(rules.get("approval_patterns", [])) + len(rules.get("rejection_patterns", []))
        total_reviews = self.feedback_data.get("learned_rules", {}).get("stats", {}).get("total_reviewed", 1)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å¯†åº¦
        pattern_density = total_patterns / max(total_reviews, 1)
        
        # ä¿¡é ¼åº¦ã®å¹³å‡
        avg_confidence = 0.0
        for patterns in [rules.get("approval_patterns", []), rules.get("rejection_patterns", [])]:
            for pattern in patterns:
                avg_confidence += pattern.get("confidence", 0.0)
        
        if total_patterns > 0:
            avg_confidence /= total_patterns
        
        return min((pattern_density * 2 + avg_confidence) / 2, 1.0)
    
    def _get_recent_learning_activity(self) -> List[Dict[str, Any]]:
        """æœ€è¿‘ã®å­¦ç¿’æ´»å‹•å–å¾—"""
        reviews = self.feedback_data.get("reviews", [])
        
        # æœ€æ–°5ä»¶ã®æ´»å‹•
        recent_reviews = sorted(reviews, key=lambda x: x.get("timestamp", ""), reverse=True)[:5]
        
        activity = []
        for review in recent_reviews:
            activity.append({
                "timestamp": review.get("timestamp"),
                "decision": review.get("decision"),
                "file_name": review.get("file_name"),
                "learning_impact": self._assess_learning_impact(review)
            })
        
        return activity
    
    def _assess_learning_impact(self, review: Dict[str, Any]) -> str:
        """å­¦ç¿’ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè©•ä¾¡"""
        characteristics = review.get("data_characteristics", {})
        
        if characteristics.get("personal_content", False):
            return "é«˜ã„å­¦ç¿’ä¾¡å€¤"
        elif characteristics.get("mixed_content", False):
            return "ä¸­ç¨‹åº¦ã®å­¦ç¿’ä¾¡å€¤"
        else:
            return "ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã¸ã®è²¢çŒ®"
    
    def _calculate_prediction_accuracy(self) -> Dict[str, Any]:
        """äºˆæ¸¬ç²¾åº¦è¨ˆç®—"""
        # ç°¡ç•¥å®Ÿè£…ï¼šå®Ÿéš›ã«ã¯å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®äºˆæ¸¬ç²¾åº¦è¨ˆç®—
        return {
            "current_accuracy": 0.915,  # 91.5%
            "prediction_confidence": 0.85,
            "improvement_trend": "positive",
            "next_milestone": "95.0%ç²¾åº¦é”æˆ"
        }
    
    def _generate_user_guidance(self) -> List[str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ç”Ÿæˆ"""
        stats = self.feedback_data.get("learned_rules", {}).get("stats", {})
        approval_rate = stats.get("approval_rate", 0)
        
        guidance = []
        
        if approval_rate < 40:
            guidance.append("ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿å“è³ªãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚å€‹äººçš„ãªä½“é¨“ã‚„æ„Ÿæƒ…ã‚’å«ã‚€å†…å®¹ã®å…¥åŠ›ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚")
        elif approval_rate < 70:
            guidance.append("å­¦ç¿’åŠ¹æœãŒè‰¯å¥½ã§ã™ã€‚æŠ€è¡“çš„å†…å®¹ã¨å€‹äººçš„æ´å¯Ÿã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã¾ã™ã€‚")
        else:
            guidance.append("å„ªç§€ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ã“ã®å“è³ªã‚’ç¶­æŒã™ã‚‹ã“ã¨ã§95%ç²¾åº¦é”æˆã«è²¢çŒ®ã—ã¾ã™ã€‚")
        
        guidance.append("ã‚·ã‚¹ãƒ†ãƒ ã¯ã‚ãªãŸã®åˆ¤æ–­ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ç¶™ç¶šçš„ã«å­¦ç¿’ã—ã€æ¨å¥¨ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¦ã„ã¾ã™ã€‚")
        guidance.append("ç•°ãªã‚‹ç¨®é¡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆæŠ€è¡“çš„ã€æ„Ÿæƒ…çš„ã€æˆ¦ç•¥çš„ï¼‰ã®å…¥åŠ›ã«ã‚ˆã‚Šã€å­¦ç¿’åŠ¹æœãŒæœ€å¤§åŒ–ã•ã‚Œã¾ã™ã€‚")
        
        return guidance


def create_feedback_visualization_system(project_root: Optional[Path] = None) -> FeedbackLearningVisualizer:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ä½œæˆ"""
    return FeedbackLearningVisualizer(project_root)


if __name__ == "__main__":
    # å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
    print("ğŸ“Š ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    visualizer = create_feedback_visualization_system()
    dashboard = visualizer.create_learning_progress_dashboard()
    
    # çµæœè¡¨ç¤º
    overview = dashboard["learning_overview"]
    print(f"âœ… ç·ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°: {overview['total_reviews']}")
    print(f"ğŸ“ˆ æ‰¿èªç‡: {overview['approval_rate']:.1f}%")
    print(f"ğŸ§  å­¦ç¿’ãƒ«ãƒ¼ãƒ«æ•°: {overview['learning_rules_count']}")
    print(f"âš¡ å­¦ç¿’åŠ¹æœ: {overview['learning_effectiveness']:.1%}")
    
    prediction = dashboard["prediction_accuracy"]
    print(f"\nğŸ¯ ç¾åœ¨ç²¾åº¦: {prediction['current_accuracy']:.1%}")
    print(f"ğŸ”® äºˆæ¸¬ä¿¡é ¼åº¦: {prediction['prediction_confidence']:.1%}")
    print(f"ğŸ“Š æ”¹å–„å‚¾å‘: {prediction['improvement_trend']}")
    
    print("\nğŸ’¡ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹:")
    for i, guidance in enumerate(dashboard["user_guidance"][:2], 1):
        print(f"  {i}. {guidance}")
    
    print("\nğŸ‰ å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†!")